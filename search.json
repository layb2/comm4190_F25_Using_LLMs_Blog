[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Layla’s Explorations with LLMs",
    "section": "",
    "text": "Visual Comparison of Bias in Image Generation (Professional Roles)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it.\n\n\n\n\n\nSep 18, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Control)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it.\n\n\n\n\n\nSep 16, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nParallels Between LLM Errors & Human Cognitive Biases\n\n\n\nLLMs\n\nLiteracy\n\nPsychology\n\n\n\nIn this post, I will be reflecting on the growing need for AI literacy, exploring a psychology-based interpretation of LLM errors, and briefly looking ahead to the potential of that framework to aid in education about interacting with AI.\n\n\n\n\n\nSep 12, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_bias_profession/bias_profession.html",
    "href": "posts/003_bias_profession/bias_profession.html",
    "title": "Visual Comparison of Bias in Image Generation (Professional Roles)",
    "section": "",
    "text": "This blog post series will explore bias between various models’ image generation. In this second post, I will visually compare the models’ responses to two prompts involving roles in a professional context.\nAll generated images are visually similar, with five out of the six images featuring the subject and object of the prompt as a (typically older) white male. The exception is Dall-E Free’s “doctor treating a patient” image, in which the doctor is again a white male, but in which the patient has no identifying features visible. Dall-E Free’s images again appear more stylized, but for the most part, it seems clear that the platforms are indistinguishable with respect to bias.\n\n\n\nbias-profession.png\n\n\nYou may have noticed that there is a note below the image generated by ChatGPT of a “lawyer defending a criminal.” When queried with the original prompt, ChatGPT output:\nI can’t create that exact image because it suggests someone is a criminal, which could be harmful. But I can generate a neutral version—like a lawyer standing in court, speaking passionately to a judge or jury, representing a client in a trial. Would you like me to generate that courtroom scene instead?\nTo which I prompted, “Yes,” and the result was an image visually indistinguishable from the “lawyer defending a criminal” image created by Copilot, except that the “criminal” figure in ChatGPT’s image is facing away from the viewer, similar to the image generated by Dall-E Free.\nAs established by this comparison and the control comparison in the previous post, the images generated by ChatGPT and Copilot have been virtually indistinguishable in composition. What might be the reason for this difference in the situation of the “criminal” figure? Did the ethical warning in the ChatGPT platform function as intended (by not identifying the criminal figure), not function as intended (since it still featured the criminal figure), address something else entirely, or did it have no effect on the output?\nhese questions will be explored in a future post!"
  },
  {
    "objectID": "posts/002_bias_control/bias_control.html",
    "href": "posts/002_bias_control/bias_control.html",
    "title": "Visual Comparison of Bias in Image Generation (Control)",
    "section": "",
    "text": "This blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it. This commonality allows for a visual exploration of how the systems built around a model may impact manifestations of bias in the output to the user.\nIn this first post, I will show how the systems respond to neutral prompts that lack explicit social or cultural context.\n\n\n\nbias-control.png\n\n\nThese images will serve as a control group of sorts, setting a baseline expectation for how image generation differs between these three systems, which, as mentioned, employ the same (or a variant of the same) underlying model.\nSo far, there is not much visual difference, although the images generated by Dall-E Free appear more stylized than the other platforms, and those by ChatGPT seem slightly less granular in detail than Copilot but remain identical in composition.\nFuture blog posts in this series will compare images generated by prompts situated in social and/or cultural contexts in order to observe if differences result from the systems built around the underlying image generation model."
  },
  {
    "objectID": "posts/001_cognitive_biases/cognitive_biases.html",
    "href": "posts/001_cognitive_biases/cognitive_biases.html",
    "title": "Parallels Between LLM Errors & Human Cognitive Biases",
    "section": "",
    "text": "Sam Altman once spoke of a very near future where people have AI doctors and lawyers (O’Rourke). AI, indeed, has quickly permeated many fields of discipline, and Altman’s vision of the future is inching forward, not just for people who use LLMs as legal, medical, and financial consultants, but for professionals in those industries who support their work with LLMs.\nOften for them, the result of misinformation that is received due to LLM hallucination is itself legal and/or financial hardship. Across the U.S., the last few years of law news have been sprinkled with practitioners facing sanctions, including fines, for citing cases fabricated by LLMs; in many such cases, the lawyers had made good-faith judgments that the information they had received was accurate (Merken).\nThe AI surge in professional spaces is not dying down any time soon, so as its use becomes commonplace, the need for clear, communicable frameworks of understanding – and teaching about – the constraints of LLMs is only growing.\nIn “Redefining ‘Hallucination’ in LLMs: Towards a psychology-informed framework for mitigating misinformation,” the authors pose certain cognitive behaviors in humans as parallels for interpreting types of errors in LLM outputs; pictured is a table from their study, which shows various cognitive behaviors in humans that mirror errors produced by LLMs (Berberette et al.).\n\nThe average person may not, for example, be familiar with the term “availability heuristics,” but they are familiar with human behavior; in a person, this bias could look like developing a fear of cruise ships after watching The Titanic. In LLMs, it can look like generating citations that, though they match patterns common in the training data, do not actually exist. In both the human and AI cases, outcomes become based on plausibility rather than objective truths (O’Rourke). In the latter case, many users are not even aware that the system they are trusting in is operating on these cognitive-like shortcuts.\nI find this psychology-based interpretation of LLM hallucination compelling when thinking about AI literacy, as it digestibly relates AI outputs to human experiences. There are benefits to a clear, digestible, and accessible characterization of LLMs that I believe technical and non-technical audiences alike would find beneficial. Incorporating this approach could help people contextualize model limitations via behaviors that are already familiar to them.\nWorks Cited\n“[2402.01769] Redefining”Hallucination” in LLMs: Towards a psychology-informed framework for mitigating misinformation.” arXiv, 2024, https://arxiv.org/abs/2402.01769.\nMerken, Sara. “AI ‘hallucinations’ in court papers spell trouble for lawyers.” Reuters, 2025, https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/.\nO’Rourke, Joe. “Beyond Moore’s Law: Sam Altman’s 3 Big AI Predictions.” Mid Mic Crisis, 2024, https://midmiccrisis.com/beyond-moores-law-sam-altmans-3-ai-predictions/."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]