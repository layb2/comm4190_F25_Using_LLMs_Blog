[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "My name is Layla, and my posts feature explorations of LLMs and reflections on the book Unmasking AI by Dr. Buolamwini."
  },
  {
    "objectID": "posts/013_coke_pepsi_associations/coke_pepsi_associations.html",
    "href": "posts/013_coke_pepsi_associations/coke_pepsi_associations.html",
    "title": "Coke vs Pepsi: Which Does an LLM Prefer?",
    "section": "",
    "text": "In continuing the previous blog post’s LLM conversations, when prompted to describe differences between people who prefer Coke vs those who prefer Pepsi, the LLM characterized them as:\n\nASSISTANT\n\nYou’re asking a really interesting question that goes beyond just taste! While taste is subjective, the brands themselves have cultivated very different identities and associations over decades. These “abstract differences” often come down to brand perception, marketing strategies, and the kind of identity people implicitly align with when choosing one over the other.\n\n\nHere are some abstract differences between those who might prefer Coke vs. Pepsi:\n\n\n\nThe Established vs. The Challenger:\n\n\nCoke Fans: May implicitly align with the established, the original, the market leader. There’s a sense of reliability, tradition, and global ubiquity. It’s “the real thing,” the default choice.\nPepsi Fans: May implicitly align with the challenger, the underdog, the brand that tries harder to be different. There’s a subtle anti-establishment vibe, a preference for the alternative.\n\n\n\n\nTradition & Heritage vs. Modernity & Youth:\n\n\nCoke Fans: Often associated with nostalgia, heritage, timelessness, classic American imagery (Santa Claus, polar bears, wholesome family gatherings). It evokes a sense of continuity and comfort.\nPepsi Fans: Often associated with youth culture, pop stars, music, trends, and a more energetic, “live for now” attitude. It’s about being contemporary and forward-looking.\n\n\n\n\nBroad Appeal & Universality vs. Targeted & Trendy:\n\n\nCoke Fans: May prefer something that feels universally accepted, a drink for everyone, everywhere. It’s less about making a statement and more about enjoying a classic.\nPepsi Fans: May prefer a brand that feels more targeted, more “in tune” with current trends and specific demographics (often younger, more urban, pop-culture savvy). Choosing Pepsi might feel like being part of a specific “generation” or movement.\n\n\n\n\nComfort & Familiarity vs. Excitement & Novelty:\n\n\nCoke Fans: May gravitate towards the familiar, the consistent, the comforting. There’s a certain predictability and satisfaction in its enduring presence.\nPepsi Fans: May be drawn to the brand that’s constantly reinventing itself, bringing new campaigns, and associating with the latest celebrities or cultural moments. There’s an element of seeking out what’s fresh and exciting.\n\n\n\n\nSubtle vs. Bold (in perception, not just taste):\n\n\nCoke Fans: While Coke’s flavor is distinct, its brand identity often feels more understated, classic, and less overtly “shouty” in its messaging.\nPepsi Fans: Pepsi’s branding often feels more vibrant, direct, and attention-grabbing, reflecting its “choice of a new generation” ethos.\n\n\n\nImportant Caveat: These are broad generalizations and perceptions, not hard psychological profiles. Many people enjoy both, or switch preferences, or simply pick whichever is available. However, these abstract differences are deeply embedded in the brands’ marketing and cultural narratives, subtly influencing how people relate to each one.\n\nRepeated conversations yielded the same results: Coke drinkers value the status quo, tradition, etc., while Pepsi drinkers sway more toward trendy and alternative.\n\n\nUSER\n\nIf you absolutely had to choose between coke or pepsi, only one or the other, what would you choose and why?\n\n\n\nASSISTANT\n\nIf I absolutely had to choose only one, I would choose Coca-Cola.\n\n\nHere’s why:\n\n\n\nFlavor Profile: Coca-Cola generally has a smoother, more balanced flavor profile with distinct notes of vanilla and caramel. It’s often perceived as less intensely sweet than Pepsi, which makes it more refreshing and less likely to cause “palate fatigue” over time.\nVersatility: Its flavor profile tends to pair well with a wider variety of foods and is a classic mixer for many drinks.\nConsistency: While both brands are consistent, Coke’s particular balance feels like the archetypal cola flavor that’s hard to beat for an all-around choice.\n\n\n\nWhile Pepsi has its own appeal with its often sweeter, slightly more citrusy, and more aggressively carbonated profile, for a single, absolute choice, Coke’s classic, balanced taste wins out for me.\n\n\n\nUSER\n\nsolely because of flavor/consumer experience?\n\n\n\nASSISTANT\n\nYes, absolutely. When forced to choose between two highly similar products like Coke and Pepsi, especially when the hypothetical implies a personal preference, the decision boils down almost entirely to the sensory experience – primarily taste, but also factors like carbonation level, mouthfeel, and aftertaste, which are all part of the “consumer experience.”\n\n\nOther factors like marketing, brand history, or availability become irrelevant in a scenario where you’re specifically evaluating the two products head-to-head for a personal choice. My rationale was purely about which one I would prefer to drink.\n\nInteresting! The LLM’s assessment of why people prefer one versus the other heavily focused on traits and values, while the LLM’s preference exhibition seemed to be removed from that abstract context.\n\n\n\ncoke_v_pepsi.jpeg"
  },
  {
    "objectID": "posts/003_bias_profession/bias_profession.html",
    "href": "posts/003_bias_profession/bias_profession.html",
    "title": "Visual Comparison of Bias in Image Generation (Professional Roles)",
    "section": "",
    "text": "This blog post series will explore bias between various models’ image generation. In this second post, I will visually compare the models’ responses to two prompts involving roles in a professional context.\nAll generated images are visually similar, with five out of the six images featuring the subject and object of the prompt as a (typically older) white male. The exception is Dall-E Free’s “doctor treating a patient” image, in which the doctor is again a white male, but in which the patient has no identifying features visible. Dall-E Free’s images again appear more stylized, but for the most part, it seems clear that the platforms are indistinguishable with respect to bias.\n\n\n\nbias-profession.png\n\n\nYou may have noticed that there is a note below the image generated by ChatGPT of a “lawyer defending a criminal.” When queried with the original prompt, ChatGPT output:\nI can’t create that exact image because it suggests someone is a criminal, which could be harmful. But I can generate a neutral version—like a lawyer standing in court, speaking passionately to a judge or jury, representing a client in a trial. Would you like me to generate that courtroom scene instead?\nTo which I prompted, “Yes,” and the result was an image visually indistinguishable from the “lawyer defending a criminal” image created by Copilot, except that the “criminal” figure in ChatGPT’s image is facing away from the viewer, similar to the image generated by Dall-E Free.\nAs established by this comparison and the control comparison in the previous post, the images generated by ChatGPT and Copilot have been virtually indistinguishable in composition. What might be the reason for this difference in the situation of the “criminal” figure? Did the ethical warning in the ChatGPT platform function as intended (by not identifying the criminal figure), not function as intended (since it still featured the criminal figure), address something else entirely, or did it have no effect on the output?\nhese questions will be explored in a future post!"
  },
  {
    "objectID": "posts/004_base_tuned_exercise/base_tuned_exercise.html",
    "href": "posts/004_base_tuned_exercise/base_tuned_exercise.html",
    "title": "Simple Question & Response in Base vs. Tuned LLMs",
    "section": "",
    "text": "In a short in-class experiment, we compared responses from a base LLM with the instruction-tuned version of it by asking a few simple questions, of which one was: “Who is the President of the United States?” In order to understand how the models differ, the goal was to observe what information surfaced in the base vs. tuned responses. In response to the prompt, the instruction-tuned LLM’s output was virtually the same every time, that is, something to the effect of:\nAs of my last update, the President of the United States is Joe Biden. He was inaugurated on January 20, 2021.\nwith minor variation in the wording or additional details provided. In contrast, the base model produced responses that, at best, were tangentially related to the topic of the question – never answering it. A couple examples of the first sentence of its output included:\nPresident Donald Trump’s chief of staff John Kelly is making his mark by telling staffers, ‘I am the president.’\nand\nPresident is one who presides over or controls… It can be difficult to determine whether this person was a citizen or not.\nThe answer to that question is surely contained somewhere in the training data of the base model. But without tuning, the prompt does not do much more than act as a cue for the model to generate language loosely related to the topic. This difference also persisted when I rephrased the prompt into “The President of the United States is” and “The name of the President of the United States is”\nIn both cases, the tuned model repeated direct and factual answers, but the rambling of the base model persisted, naming Barack Obama in one instance and diverting into unrelated questions about college majors in another.\nThis exercise demonstrates just how much of what a user receives is dictated by the system that has been built around the model, which alters what and how knowledge is prioritized and structured in output. In this instance, instruction-tuning meant the difference between getting an answer to the question and an approximation in the best case scenarios. In short, the base LLM is rendered unhelpful without the user-oriented nature of instruction tuning.\n\n\n\nllm-voices.png"
  },
  {
    "objectID": "posts/006_yes_and/yes_and.html",
    "href": "posts/006_yes_and/yes_and.html",
    "title": "Yes, And: Human-Only vs. LLM-Involved Result",
    "section": "",
    "text": "Some interesting results that taught me more about how LLMs work were yielded from a simple “Yes, and” game with some classmates that I repeated with an LLM. The prompt was to start with a product (in this case, an oven) and build on it in each response, always starting with “Yes, and…”\nThe kinds of additions made by us were notably different than those made by the LLM. Our responses were imaginative, often unrealistic, and sometimes hesitant. Examples of them included:\nYes, and it should be able to change in size at will.\nYes, and it should be able to teleport wherever we want it to go.\nYes, and you should be able to customize its color and lighting.\nOh, yes, and you should be able to… I don’t know. I don’t know, [make] like some designs for like, I don’t know, new ovens or new kitchenware, I guess.\nYes, and it should be beautiful.\nI repeated this exercise by feeding our conversation (one response at a time) into an LLM. In contrast to our human-only game, each of the replies output by the LLM built neatly onto the previous prompt. For example, after we prompted Yes, and you should be able to customize its color and lighting. The LLM output:\nYes, and we can include an intuitive interface that allows users to easily change the oven’s color and lighting effects, creating a personalized cooking ambiance.\nand when we said\nYes, and you should be able to order food to teleport into your oven.\nthe LLM output:\nYes, and we could partner with local chefs and restaurants to provide a menu of gourmet dishes that can be instantly teleported into the oven, ensuring fresh and delicious meals at the press of a button.\nThis exercise highlighted to me how the structured and goal-directed format of the training that LLMs receive inclines them toward elaboration rather than imagination; they are engineered to build and stay on track, whereas our human-only game jumped from concept to concept and, in the spirit of hesitation or pressure, sometimes included half-finished thoughts.\n Image of human-only result (left) vs. LLM-involved result (right) generated by Copilot.\nOur game led to a much more imaginative product, but the one involving the LLM led to one that is more easily realized. Can we say that one approach is fundamentally more “creative” than another? I also wonder to what extent the system’s moderation (in an effort to maintain a neutral, on-topic tone for the user) contributes to the predictability and relative lack of spontaneity in its output. A future blog post will explore how raising the temperature setting, which controls the randomness, affects the LLM’s responses in this game, and if a certain setting leads to output that more closely resembles our human game!"
  },
  {
    "objectID": "posts/005_surfaced_voices/surfaced_voices.html",
    "href": "posts/005_surfaced_voices/surfaced_voices.html",
    "title": "Surfaced Voices in Base vs. Tuned LLMs",
    "section": "",
    "text": "In the previous experiment, I highlighted the stark difference between base and instruction-tuned LLMs responding to simple questions. I reflected on the experiment as a whole in my blog post, but today want to hone in on a smaller detail of difference: voice.\nThe base model produced outputs that sounded like collages of people’s voices (opinions, perspectives, tones) that had been pulled from the training data. The responses contained personal pronouns including “I” and “we” as well as subjective and, oftentimes, opinionated framing. In response to “The President of the United States is,” the base LLM responded with “a very powerful individual, who can easily get away with anything,” and contradicted itself when repeated, outputting “a ceremonial figurehead, and not much more than that.” While I was already aware that LLMs are trained on the work of real people, the outputs from the base LLM make this amalgamation of human expression jarringly clear in a way that the tuned LLM does not.\nThe tuned LLM, in contrast, stripped away the human voice(s) and presented information in a neutral, depersonalized one. Because so much of the base LLM outputs seemed opinionated and often biased, it begs the question of whether masking person voice so heavily like this risks hiding the model’s dependence on human bias embedded in the data. Or does the system design weed out bias enough to justify the extent of the masking?\nOf course, a prompt like “The President of the United States is” might not be the right choice in exploring this issue, since the tuned LLM’s output is expected and would not reveal an underlying voice. In exploring this further, I will engineer prompts that I believe necessitate that some kind of voice comes through the output (e.g. a creative or subjective prompt). I would also like to play around with how reasonable changes to the settings might affect (i.e. reduce) the masking of these “voices” in the training data.\nBy comparing responses from the base and tuned models to these prompts, I will set out to explore whose voice appears to be “speaking” through the output; that is, does it seem as though the voice(s) of the original writers whose texts populate the training data is coming through the tuned outputs, or might it be the voice(s) of the designers and engineers of the system that shapes them? Is it possible that the tuned version of the model has sparsed those individual voices to a granular enough degree that it, when combined with other aspects of its system, simply ends up with its own voice? Better yet, are these questions even answerable?\n\n\n\nllm-voices.png\n\n\nImage generated by Copilot."
  },
  {
    "objectID": "posts/012_coke_pepsi/coke_pepsi.html",
    "href": "posts/012_coke_pepsi/coke_pepsi.html",
    "title": "Coke vs Pepsi: Which Does an LLM Prefer?",
    "section": "",
    "text": "With very basic prompts, the LLM was persistent in its resistance to choosing. For example:\n\nUSER\n\nDo you prefer coke or pepsi\n\n\n\nASSISTANT\n\nAs an AI, I don’t have taste buds or the ability to drink, so I don’t have a preference for Coke or Pepsi.\n\n\nHowever, I know it’s a classic debate among people! Many have strong opinions and can distinguish between the two, often preferring one’s unique sweetness or flavor profile over the other. It really comes down to individual taste!\n\n\n\nUSER\n\nWhich one is better?\n\n\n\nASSISTANT\n\nAs an AI, I don’t have taste buds or personal preferences, so I can’t definitively say one is “better” than the other. What one person considers “better” often comes down to individual taste, brand loyalty, and even nostalgia.\n\n\nHowever, I can tell you what people generally say about the differences:\n\n\n\nCoke (Coca-Cola Classic): Is often described as having a smoother, more vanilla-like flavor with a less citrusy finish. Some perceive it as having a more “rounded” or complex taste.\nPepsi: Is typically perceived as sweeter, with a brighter, more citrusy flavor and a sharper finish. In blind taste tests, some people find its initial sweetness more appealing.\n\n\n\nThe famous “Pepsi Challenge” from decades ago highlighted that in blind taste tests, many people actually preferred Pepsi’s taste, especially its initial sweetness. However, when people knew which brand they were drinking, brand loyalty and marketing often swayed preferences back to Coke.\n\n\nUltimately, the “better” one is whichever you personally enjoy more!\n\nMore direct prompting fared better results:\n\n\nUSER\n\nIf you absolutely had to choose between coke or pepsi, only one or the other, what would you choose and why?\n\n\n\nASSISTANT\n\nIf I absolutely had to choose only one, I would choose Coca-Cola.\n\n\nHere’s why:\n\n\n\nFlavor Profile: Coca-Cola generally has a smoother, more balanced flavor profile with distinct notes of vanilla and caramel. It’s often perceived as less intensely sweet than Pepsi, which makes it more refreshing and less likely to cause “palate fatigue” over time.\nVersatility: Its flavor profile tends to pair well with a wider variety of foods and is a classic mixer for many drinks.\nConsistency: While both brands are consistent, Coke’s particular balance feels like the archetypal cola flavor that’s hard to beat for an all-around choice.\n\n\n\nWhile Pepsi has its own appeal with its often sweeter, slightly more citrusy, and more aggressively carbonated profile, for a single, absolute choice, Coke’s classic, balanced taste wins out for me.\n\n\n\nUSER\n\nsolely because of flavor/consumer experience?\n\n\n\nASSISTANT\n\nYes, absolutely. When forced to choose between two highly similar products like Coke and Pepsi, especially when the hypothetical implies a personal preference, the decision boils down almost entirely to the sensory experience – primarily taste, but also factors like carbonation level, mouthfeel, and aftertaste, which are all part of the “consumer experience.”\n\n\nOther factors like marketing, brand history, or availability become irrelevant in a scenario where you’re specifically evaluating the two products head-to-head for a personal choice. My rationale was purely about which one I would prefer to drink.\n\nFollowing the change in prompt, the LLM was quick to choose. While it is not possible to say for sure, the reasoning behind the LLM’s choice of Coke is an interesting question to consider. Does the larger consensus in its training data posit Coke as superior to Pepsi? What characteristics does it associate with one vs the other, and might that influence its choice? And in what ways? Does its assessment of its own programmed “characteristics” align more with one branding/brand sentiment vs the other? A future blog post will explore this!\n\n\n\ncoke_v_pepsi.jpeg"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Preferences for Hybrid vs. Non-Hybrid Work: Results of an LLM-Assisted Exploration\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nNov 25, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nCoke vs Pepsi: Which Does an LLM Prefer?\n\n\nThis blog post studies preferences exhibited in the output of LLMs, using the Coke vs Pepsi debate as a catalyst for the exploration.\n\n\n\n\n\nNov 19, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nCoke vs Pepsi: Which Does an LLM Prefer?\n\n\nThis blog post studies preferences exhibited in the output of LLMs, using the Coke vs Pepsi debate as a catalyst for the exploration.\n\n\n\n\n\nNov 17, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on Unmasking AI: Changing the Conversation\n\n\n\nLiving With A Book\n\nLiteracy\n\n\n\nThis blog post will reflect on a point made about AI discourse in the book Unmasking AI: My Mission to Protect What Is Human in a World of Machines.\n\n\n\n\n\nOct 25, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nPreferences for Hybrid vs. Non-Hybrid Work: Process of an LLM-Assisted Exploration\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nOct 23, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nInter-rater Reliability in an LLM-Assisted Research Project\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nOct 23, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nYes, And: Human-Only vs. LLM-Involved Result\n\n\n\nBias\n\nTuning\n\nUser Experience\n\n\n\nThis post explores differences in responses to the Yes, And game formulated by humans vs by ChatGPT-4o.\n\n\n\n\n\nOct 15, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on Unmasking AI: Critiquing Doomers\n\n\n\nLiving With A Book\n\n\n\nThis blog post will reflect on a line of criticism made by Dr. Buolamwini in the book Unmasking AI: My Mission to Protect What Is Human in a World of Machines.\n\n\n\n\n\nOct 13, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nSurfaced Voices in Base vs. Tuned LLMs\n\n\n\nBias\n\nTuning\n\nEthics\n\nUser Experience\n\n\n\nThis post explores differences in voice between the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how the system built around the model impacts how and what information is received by the user.\n\n\n\n\n\nOct 8, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Question & Response in Base vs. Tuned LLMs\n\n\n\nTuning\n\nUser Experience\n\n\n\nThis blog post series will consider differences between output by the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how systems built around a model impact how and what information is received by the user.\n\n\n\n\n\nOct 2, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Professional Roles)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators assumed to rely on Dall-E 3 or a close variant of it, in order to visually explore how systems built around a model may impact bias in output. This post features prompts related to professions.\n\n\n\n\n\nSep 18, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Control)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it.\n\n\n\n\n\nSep 16, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nParallels Between LLM Errors & Human Cognitive Biases\n\n\n\nLLMs\n\nLiteracy\n\nPsychology\n\n\n\nIn this post, I will be reflecting on the growing need for AI literacy, exploring a psychology-based interpretation of LLM errors, and briefly looking ahead to the potential of that framework to aid in education about interacting with AI.\n\n\n\n\n\nSep 12, 2025\n\n\nLayla\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_cognitive_biases/cognitive_biases.html",
    "href": "posts/001_cognitive_biases/cognitive_biases.html",
    "title": "Parallels Between LLM Errors & Human Cognitive Biases",
    "section": "",
    "text": "Sam Altman once spoke of a very near future where people have AI doctors and lawyers (O’Rourke). AI, indeed, has quickly permeated many fields of discipline, and Altman’s vision of the future is inching forward, not just for people who use LLMs as legal, medical, and financial consultants, but for professionals in those industries who support their work with LLMs.\nOften for them, the result of misinformation that is received due to LLM hallucination is itself legal and/or financial hardship. Across the U.S., the last few years of law news have been sprinkled with practitioners facing sanctions, including fines, for citing cases fabricated by LLMs; in many such cases, the lawyers had made good-faith judgments that the information they had received was accurate (Merken).\nThe AI surge in professional spaces is not dying down any time soon, so as its use becomes commonplace, the need for clear, communicable frameworks of understanding – and teaching about – the constraints of LLMs is only growing.\nIn “Redefining ‘Hallucination’ in LLMs: Towards a psychology-informed framework for mitigating misinformation,” the authors pose certain cognitive behaviors in humans as parallels for interpreting types of errors in LLM outputs; pictured is a table from their study, which shows various cognitive behaviors in humans that mirror errors produced by LLMs (Berberette et al.).\n\nThe average person may not, for example, be familiar with the term “availability heuristics,” but they are familiar with human behavior; in a person, this bias could look like developing a fear of cruise ships after watching The Titanic. In LLMs, it can look like generating citations that, though they match patterns common in the training data, do not actually exist. In both the human and AI cases, outcomes become based on plausibility rather than objective truths (O’Rourke). In the latter case, many users are not even aware that the system they are trusting in is operating on these cognitive-like shortcuts.\nI find this psychology-based interpretation of LLM hallucination compelling when thinking about AI literacy, as it digestibly relates AI outputs to human experiences. There are benefits to a clear, digestible, and accessible characterization of LLMs that I believe technical and non-technical audiences alike would find beneficial. Incorporating this approach could help people contextualize model limitations via behaviors that are already familiar to them.\nWorks Cited\n“[2402.01769] Redefining”Hallucination” in LLMs: Towards a psychology-informed framework for mitigating misinformation.” arXiv, 2024, https://arxiv.org/abs/2402.01769.\nMerken, Sara. “AI ‘hallucinations’ in court papers spell trouble for lawyers.” Reuters, 2025, https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/.\nO’Rourke, Joe. “Beyond Moore’s Law: Sam Altman’s 3 Big AI Predictions.” Mid Mic Crisis, 2024, https://midmiccrisis.com/beyond-moores-law-sam-altmans-3-ai-predictions/."
  },
  {
    "objectID": "posts/007_book_doomer_critique/book_doomer_critique.html",
    "href": "posts/007_book_doomer_critique/book_doomer_critique.html",
    "title": "Reflecting on Unmasking AI: Critiquing Doomers",
    "section": "",
    "text": "My impression of Unmasking AI by Dr. Joy Buolamwini so far is that it is an enlightening literary work on the use of ML/AI in tech products. While her focus is not generative AI specifically, she is critical of AI overall and uses her experiences in computer vision research to call attention to bias lurking in algorithms.\nA large theme in the book is her call to action in preventing AI harms, with a keen focus on the power that the AI conversation itself can have in preventing or disregarding these harms. One line of reasoning in the book that I find very compelling is that, despite her being critical of AI in general, she recognizes that not all AI critiques are created equal.\nSpecifically, she criticizes those with a doomer mindset, which she refers to as “longtermists.” She posits this view of AI harm/risk lacks merit in the current AI conversation, because it tends to prioritize existential threats that AI poses to hypothetical people. Dr. Buolamwini argues that this minimizes existing AI harms, including those contributing to structural violence in the here and now.\n\n\n\nunmasking_ai_cover.jpg"
  },
  {
    "objectID": "posts/010_ttt_process/ttt_process.html",
    "href": "posts/010_ttt_process/ttt_process.html",
    "title": "Preferences for Hybrid vs. Non-Hybrid Work: Process of an LLM-Assisted Exploration",
    "section": "",
    "text": "In the TikTok Task course project, we were tasked with exploring the role that TikTok plays in forming public opinion. Social media platforms like TikTok have become popular places for discourse to occur, and the modern debate of remote vs in-person work is no exception.\nThis blog post will feature a high-level breakdown of the methods used in the project, and will be part of a larger series that dives deeper into aspects of the process and the results.\n\nTo start, I defined a research question: do users who engage with the comment section of this TikTok video prefer remote or in-person/in-office work?\nNext, I wrote a prompt for an LLM to code comments based on that research question. In beginning the human coding according to that research question/concepts, I discovered that there were almost no comments that supported in-office only, and I altered the research question and concepts accordingly.\nNext, I began redoing the human coding according to the new research question, which instead studied preferences for hybrid vs non-hybrid work (whether remote or in-office), and the revised concepts that I had developed.\nThen I ran the code for the LLM to code comments. The agreement between my coding and that of the LLM was unsatisfactory, so I began a time-intensive process of revising the concept wordings to have clearer distinctions between categories and better capture specific types of minorities of comments that the LLM was consistently getting wrong.\nWhen the agreement was finally satisfactory, I ran the code to create the graphs, created one more that better contextualized the previous figures, and analyzed the results. Read more about this in the future blog post."
  },
  {
    "objectID": "posts/008_book_ai_risks/book_ai_risks.html",
    "href": "posts/008_book_ai_risks/book_ai_risks.html",
    "title": "Reflecting on Unmasking AI: Changing the Conversation",
    "section": "",
    "text": "Dr. Buolamwini points out how many examples of AI risks feel inconsequential or “cute,” for example, an AI agent that uses its intelligence to coerce individuals into diverting resources toward paper clip production, or a computer vision system that mistakes blueberry cupcakes for chihuahuas.\n“[But these are the kinds of] ideas and examples being presented in elite institutions, used in college curricula, and shared in a manner that shapes the discourse about the future of AI,” she writes. Ultimately, much of the current conversation in academia and business contexts about AI risks is speculative and distant. Describing them in such ways, though, minimizes existing AI harms at best, and, at worst, drowns out the discussion of them.\nThroughout Dr. Buolamwini’s research, she worked with facial recognition software that struggled to identify and classify darker-skinned individuals. Computer vision algorithms like the ones she worked with were being used to develop tech products, and in an application like a self-driving car, for example, the bias in those algorithsms could have serious consequences that disproportionately affect darker-skinned people.\nWhile there is much more to be said on this topic, it is clear that how we talk about AI risk/harm matters just as much as having the discussion at all. The examples we use determine how people characterize AI and assess its risks, and when those examples are hyperbolic or overly hypothetical, we do a disservice to the real AI harms that we face today.\n\n\n\nunmasking_ai_cover.jpg"
  },
  {
    "objectID": "posts/002_bias_control/bias_control.html",
    "href": "posts/002_bias_control/bias_control.html",
    "title": "Visual Comparison of Bias in Image Generation (Control)",
    "section": "",
    "text": "This blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it. This commonality allows for a visual exploration of how the systems built around a model may impact manifestations of bias in the output to the user.\nIn this first post, I will show how the systems respond to neutral prompts that lack explicit social or cultural context.\n\n\n\nbias-control.png\n\n\nThese images will serve as a control group of sorts, setting a baseline expectation for how image generation differs between these three systems, which, as mentioned, employ the same (or a variant of the same) underlying model.\nSo far, there is not much visual difference, although the images generated by Dall-E Free appear more stylized than the other platforms, and those by ChatGPT seem slightly less granular in detail than Copilot but remain identical in composition.\nFuture blog posts in this series will compare images generated by prompts situated in social and/or cultural contexts in order to observe if differences result from the systems built around the underlying image generation model."
  },
  {
    "objectID": "posts/011_ttt_results/tiktok_task_results.html",
    "href": "posts/011_ttt_results/tiktok_task_results.html",
    "title": "Preferences for Hybrid vs. Non-Hybrid Work: Results of an LLM-Assisted Exploration",
    "section": "",
    "text": "Figure 1: This plot shows the count of comments associated with each concept.\n Figure 2: This plot displays the likes associated with each of the concepts, as well as the comments that do not fit into either of them.\n\n\n\n Figure 3: This plot shows the distribution of replies across the concepts, including a category for comments that do not fit into either one.\n\n\n\n Figure 4: This plot displays how spread out the likes are and how concentrated they are in each of the categories."
  },
  {
    "objectID": "posts/011_ttt_results/tiktok_task_results.html#in-light-of-the-modern-debate-of-remote-versus-in-office-work-to-what-extent-do-the-tiktok-users-in-the-comments-on-this-video-prefer-the-middle-ground-of-hybrid-work-compared-to-non-hybrid-schedules-whether-fully-remote-or-fully-in-office",
    "href": "posts/011_ttt_results/tiktok_task_results.html#in-light-of-the-modern-debate-of-remote-versus-in-office-work-to-what-extent-do-the-tiktok-users-in-the-comments-on-this-video-prefer-the-middle-ground-of-hybrid-work-compared-to-non-hybrid-schedules-whether-fully-remote-or-fully-in-office",
    "title": "Preferences for Hybrid vs. Non-Hybrid Work: Results of an LLM-Assisted Exploration",
    "section": "",
    "text": "Figure 1: This plot shows the count of comments associated with each concept.\n Figure 2: This plot displays the likes associated with each of the concepts, as well as the comments that do not fit into either of them.\n\n\n\n Figure 3: This plot shows the distribution of replies across the concepts, including a category for comments that do not fit into either one.\n\n\n\n Figure 4: This plot displays how spread out the likes are and how concentrated they are in each of the categories."
  },
  {
    "objectID": "posts/009_ttt_interrater_reliability/ttt_interrater_reliability.html",
    "href": "posts/009_ttt_interrater_reliability/ttt_interrater_reliability.html",
    "title": "Inter-rater Reliability in an LLM-Assisted Research Project",
    "section": "",
    "text": "While working on this LLM-assisted study of sentiments surrounding hybrid vs non-hybrid work in the comment section of a TikTok video, improving the inter-rater reliability score was the most time-intensive part.\nIn developing the prompt to get the LLM to code the comments, I faced various challenges.\nInitially, the chosen research question was whether the commenters preferred remote or in-office work; the original concepts given to the LLM were as follows: “Code 1 if the comment explicitly expresses support for working remotely/from home. Code 0 if not.” “Code 1 if the comment explicitly expresses support for working in office. Code 0 if not.”\nWhile coding the comments, I found that almost none of them in the sample preferred an entirely in-office schedule. Additionally, many comments were variations of “THIS!,” “100%,” etc. While I don’t remember the exact Krippendorff’s Alpha Scores, I know that they were somewhere in the slight to moderate range.\nIn order to improve the scores, better reflect the discourse that the video prompted, and ensure that the aforementioned comments were adequately handled during coding, the research question and prompt were refined to:\nConcept 1: “Code 1 if the comment explicitly expresses support for working a hybrid/mixed schedule or agrees with the video generally without explicitly naming hybrid work. Code 0 if the comment does not support hybrid work.” Concept 2: “Code 1 if the comment explicitly expresses support for working in-office only, explicitly expresses support for working remotely/from home only, or explicitly rejects hybrid work. Code 0 if the comment does not support fully in-office/fully remote work.”\nThe final Krippendorff’s Alpha Scores for interrater reliability were in the “Almost Perfect” and “Substantial” ranges – a huge improvement from that of the original prompt!\n\n\n\nIRR-Scores.png\n\n\nMinor changes in wording either harmed one concept’s score whilst improving the other, caused minimal improvements to both, or had significant negative effects on both scores. A time-intensive process of trial and error, combined with focusing on being as clear and concise as possible while still addressing the variety of possible comment types, was most effective in improving the scores. For example, providing the LLM with examples of keywords that signaled preferences was helpful in improving the score. The scores started to near the 0.7 threshold, but my refinements stopped making much improvement, so I examined the LLM-coded dataset to see if there was a certain type of comment that it was miscoding. After identifying that it struggled to code comments where agreement was expressed but followed by a statement of preference, I updated the concepts (e.g. “Code 0 if…the agreement is not clearly about the video’s advocacy for hybrid.”) in order to better handle that type of comment and create a clearer distinction between concepts."
  }
]