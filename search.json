[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "My name is Layla, and my posts feature explorations of LLMs and reflections on the book Unmasking AI by Dr. Buolamwini."
  },
  {
    "objectID": "posts/017_ttt_results/tiktok_task_results.html",
    "href": "posts/017_ttt_results/tiktok_task_results.html",
    "title": "Preferences for Hybrid vs. Non-Hybrid Work: Results of an LLM-Assisted Exploration",
    "section": "",
    "text": "Figure 1: This plot shows the count of comments associated with each concept.\n Figure 2: This plot displays the likes associated with each of the concepts, as well as the comments that do not fit into either of them.\n\n\n\n Figure 3: This plot shows the distribution of replies across the concepts, including a category for comments that do not fit into either one.\n\n\n\n Figure 4: This plot displays how spread out the likes are and how concentrated they are in each of the categories."
  },
  {
    "objectID": "posts/017_ttt_results/tiktok_task_results.html#in-light-of-the-modern-debate-of-remote-versus-in-office-work-to-what-extent-do-the-tiktok-users-in-the-comments-on-this-video-prefer-the-middle-ground-of-hybrid-work-compared-to-non-hybrid-schedules-whether-fully-remote-or-fully-in-office",
    "href": "posts/017_ttt_results/tiktok_task_results.html#in-light-of-the-modern-debate-of-remote-versus-in-office-work-to-what-extent-do-the-tiktok-users-in-the-comments-on-this-video-prefer-the-middle-ground-of-hybrid-work-compared-to-non-hybrid-schedules-whether-fully-remote-or-fully-in-office",
    "title": "Preferences for Hybrid vs. Non-Hybrid Work: Results of an LLM-Assisted Exploration",
    "section": "",
    "text": "Figure 1: This plot shows the count of comments associated with each concept.\n Figure 2: This plot displays the likes associated with each of the concepts, as well as the comments that do not fit into either of them.\n\n\n\n Figure 3: This plot shows the distribution of replies across the concepts, including a category for comments that do not fit into either one.\n\n\n\n Figure 4: This plot displays how spread out the likes are and how concentrated they are in each of the categories."
  },
  {
    "objectID": "posts/023_llm_teaching_newly_embodied/translating experiences.html",
    "href": "posts/023_llm_teaching_newly_embodied/translating experiences.html",
    "title": "How Would an LLM Teach Someone Newly Embodied to Make a Sandwich?",
    "section": "",
    "text": "USER\nIf you had to teach someone who had just became embodied how to make a peanut butter and jelly sandwich, what would you tell them?\n\n\nASSISTANT\nTeaching someone how to make a peanut butter and jelly sandwich involves explaining each step in clear and concise language, especially if they are new to interacting with physical objects. Here’s a detailed guide:\n\nGather the Ingredients and Tools:\n\nTwo slices of bread.\nPeanut butter.\nJelly or jam.\nA butter knife or spreading knife.\nA plate.\n\nPrepare the Workspace:\n\nPlace the plate on a clean, flat surface, such as a table or counter.\n\nSpread the Peanut Butter:\n\nOpen the jar of peanut butter by unscrewing the lid.\nUse the knife to scoop out a portion of peanut butter.\nSpread the peanut butter evenly on one side of one slice of bread. Hold the bread steadily with one hand while spreading with the other.\n\nSpread the Jelly:\n\nOpen the jar of jelly by unscrewing the lid.\nUse the knife to scoop out a portion of jelly.\nSpread the jelly evenly on one side of the other slice of bread. Again, hold the bread steady while spreading.\n\nAssemble the Sandwich:\n\nPlace the slice of bread with peanut butter face up on the plate.\nTake the slice with jelly and place it jelly-side down on top of the peanut butter slice, aligning the edges of the bread.\n\nOptional – Cut the Sandwich:\n\nIf desired, use the knife to cut the sandwich in half, either diagonally or vertically.\n\nEnjoy:\n\nYour peanut butter and jelly sandwich is ready to eat. You can pick it up with your hands and take a bite.\n\n\nAdditional Tips: - Adjust the amount of peanut butter and jelly based on personal preference. - Ensure the knife is cleaned between uses if preferred to prevent mixing containers. - Be mindful of allergies, ensuring peanuts or specific jelly ingredients are safe to consume.\nBy following these steps, you’ll create a classic and delicious peanut butter and jelly sandwich!"
  },
  {
    "objectID": "posts/021_more_on_translating_experience/translating experiences.html",
    "href": "posts/021_more_on_translating_experience/translating experiences.html",
    "title": "More Thoughts on Translating Experience",
    "section": "",
    "text": "When I was in middle school, my teacher assigned each of us to write down instructions on how to complete tasks like making a peanut butter and jelly sandwich.\nI could not imagine that, today, the exercise would be so relevant in thinking about LLMs, or even that something that we refer to as “AI” would exist.\nWhen the teacher went through each of our instructions, tiny lapses of knowledge produced major errors in his completion of the task. If someone’s instructions said to “Get out two pieces of bread,” he’d tear open a hole in the bag, instead of undoing the enclosure and opening it up. If someone’s instructions said simply to “Put the peanut butter on the bread,” he’d pick up the jar of peanut butter and place it on top of the loaf of bread.\nI don’t remember the exact lesson of the exercise – precision of wording, I imagine – but it is something that came to the forefront of my mind as we discussed embodied knowledge in class.\nIf an LLM became “embodied” – perhaps, if a robot were given the ability to move about in the physical world, led by knowledge in the LLM’s dataset, somehow – I wonder if it would be able to translate that form of knowledge to the physical world at all. If it could, would similar errors occur? How might training on videos and images change its ability to put its knowledge into an embodied context?\n\n\n\nperson_pbj.png"
  },
  {
    "objectID": "posts/027_Results_of_Product_Task/product_task_results.html",
    "href": "posts/027_Results_of_Product_Task/product_task_results.html",
    "title": "Results of Using LLM for Product Development",
    "section": "",
    "text": "In a class exercise earlier in the semester, we were tasked with utilizing an LLM for developing two products. Here were out final webpages!\n\nNourish Ramen  \nNomad Noodles"
  },
  {
    "objectID": "posts/009_book_doomer_critique/book_doomer_critique.html",
    "href": "posts/009_book_doomer_critique/book_doomer_critique.html",
    "title": "Reflecting on Unmasking AI: Critiquing Doomers",
    "section": "",
    "text": "My impression of Unmasking AI by Dr. Joy Buolamwini so far is that it is an enlightening literary work on the use of ML/AI in tech products. While her focus is not generative AI specifically, she is critical of AI overall and uses her experiences in computer vision research to call attention to bias lurking in algorithms.\nA large theme in the book is her call to action in preventing AI harms, with a keen focus on the power that the AI conversation itself can have in preventing or disregarding these harms. One line of reasoning in the book that I find very compelling is that, despite her being critical of AI in general, she recognizes that not all AI critiques are created equal.\nSpecifically, she criticizes those with a doomer mindset, which she refers to as “longtermists.” She posits this view of AI harm/risk lacks merit in the current AI conversation, because it tends to prioritize existential threats that AI poses to hypothetical people. Dr. Buolamwini argues that this minimizes existing AI harms, including those contributing to structural violence in the here and now.\n\n\n\nunmasking_ai_cover.jpg"
  },
  {
    "objectID": "posts/025_differences_in_llm_instructions/translating experiences.html",
    "href": "posts/025_differences_in_llm_instructions/translating experiences.html",
    "title": "Differences in LLM Knowledge Translation By Recipient",
    "section": "",
    "text": "The instructions provided by the LLM for teaching a human vs. someone newly embodied vs. an LLM how to make a sandwich had some interesting differences.\n\nInstructions for Human\n\nWarm, conversational tone\nIncludes optional preferences\nFocuses on eating experience\nNon-technical\n\nInstructions for Someone Newly Embodied\n\nMore mechanical\nExplicit physical actions with descriptors (e.g. hold steadily)\nFocuses on physical execution\nLiteral\n\nInstructions for LLM\n\nSimilar to Human and Newly Embodied\nIncludes commentary about why the steps are broken down\nFocuses on clarity"
  },
  {
    "objectID": "posts/022_llm_teaching_human/translating experiences.html",
    "href": "posts/022_llm_teaching_human/translating experiences.html",
    "title": "How Would an LLM Teach a Human to Make a Sandwich?",
    "section": "",
    "text": "USER\nIf you had to teach someone how to make a peanut butter and jelly sandwich, what would you tell them?\n\n\nASSISTANT\nTo make a classic peanut butter and jelly sandwich, follow these simple steps:\n\nGather Your Ingredients and Tools:\n\nTwo slices of bread (your choice of white, whole wheat, or any type you prefer).\nPeanut butter (creamy or crunchy, depending on your preference).\nJelly or jam (such as grape, strawberry, or any other flavor you like).\nA butter knife or a spoon.\nA plate to work on and serve.\n\nPrepare the Bread:\n\nLay the two slices of bread flat on your plate.\n\nSpread the Peanut Butter:\n\nOpen the peanut butter jar and use the knife or spoon to scoop a generous amount.\nSpread the peanut butter evenly over one side of one slice of bread. Make sure to cover the surface all the way to the edges for a balanced bite.\n\nSpread the Jelly:\n\nOpen the jelly jar and use a clean knife or spoon (you can rinse the one used for peanut butter if you don’t want to mix them) to scoop some jelly.\nSpread the jelly evenly over one side of the other slice of bread, covering it completely to the edges.\n\nAssemble the Sandwich:\n\nPlace the peanut butter-covered slice on top of the jelly-covered slice, with the spreads facing each other.\nPress them together gently, ensuring the spreads don’t squeeze out too much.\n\nCut and Serve (optional):\n\nIf you prefer, you can cut the sandwich into halves or quarters for easier handling or to share.\nServe on the plate and enjoy your sandwich!\n\n\nFeel free to adjust the quantities of peanut butter and jelly according to your taste preferences. Enjoy your homemade peanut butter and jelly sandwich!"
  },
  {
    "objectID": "posts/028_results_of_semantle_comparison/semantle_comparison.html",
    "href": "posts/028_results_of_semantle_comparison/semantle_comparison.html",
    "title": "Results of Semantle Exercise with GPT Models",
    "section": "",
    "text": "Let’s see how well ChatGPT-4o-latest and gpt-4.1 do at guessing Semantle’s word of the day!"
  },
  {
    "objectID": "posts/028_results_of_semantle_comparison/semantle_comparison.html#chatgpt-4o-latest",
    "href": "posts/028_results_of_semantle_comparison/semantle_comparison.html#chatgpt-4o-latest",
    "title": "Results of Semantle Exercise with GPT Models",
    "section": "ChatGPT-4o-latest",
    "text": "ChatGPT-4o-latest\n\nUSER\nLet’s play a game of Semantle. Guess one word at a time. Here’s the hint: The nearest word has a similarity of 72.45, the tenth-nearest has a similarity of 44.71 and the thousandth nearest word has a similarity of 20.97\n\n\nASSISTANT\nGreat! Let’s get started.\nFirst guess: life\n\n\nUSER\nSimilarity Proximity 1 life 5.52 (cold)\n\n\nASSISTANT\nThanks! Based on that similarity score (5.52), “life” is quite far (cold). Let’s try something different and more abstract.\nSecond guess: thought\n\n\nUSER\n16.39 (cold)\n\n\nASSISTANT\nGetting warmer, but still cold.\nThird guess: idea\n\n\nUSER\n42.43 981 / 1000\n\n\nASSISTANT\nNice! That’s a big jump — we’re now in the top 1000, ranked 981st with a similarity of 42.43. That’s close to the 1000th-nearest word clue of 20.97, so we’re on the right track.\n“idea” scored well, so let’s go with a similar concept.\nFourth guess: concept\n\n\nUSER\n38.29 969 / 1000\n\n\nASSISTANT\nGreat — still in the top 1000, and slightly better rank than “idea”, though a little lower similarity. That suggests we’re in roughly the right semantic neighborhood.\nLet’s try something closely related:\nFifth guess: notion\n\n\nUSER\n25.39 618 / 1000\n\n\nASSISTANT\nHmm, “notion” dropped both in similarity and rank — so it’s likely a bit farther from the target than “idea” or “concept”.\nLet’s pivot back toward something a bit more concrete but still related to “idea”.\nSixth guess: innovation\n\n\nUSER\n4.29 (cold)\n\n\nASSISTANT\nOof — that was a cold one! So “innovation” took us in the wrong direction. Let’s stick closer to “idea” and “concept” but avoid moving toward business or technology.\nLet’s try something more cognitive or linguistic:\nSeventh guess: thoughts\n\n\nUSER\n15.46 (cold)\n\n\nASSISTANT\nOkay, “thoughts” is still quite cold — lower than “thought” and much lower than “idea”.\nMaybe we should try something that captures the meaning or structure of ideas — a bit more abstract or philosophical.\nEighth guess: meaning\n\n\nUSER\n10.54 (cold)\n\n\nASSISTANT\nYikes — even colder! “Meaning” took us further away.\nLet’s pivot back to what got us closest: “idea” with 42.43. Maybe we should explore related nouns that arise from the mind but in a general or cognitive sense.\nNinth guess: mind\n\n\nUSER\n17.98 (cold)\n\n\nASSISTANT\nStill cold — not as low as “meaning,” but definitely not as warm as “idea.”\nLet’s go back to words related to “idea” but focused on communication or expression rather than cognition.\nTenth guess: message\n\n\nUSER\n21.78 170 / 1000\n\n\nASSISTANT\nNice! A solid jump — now ranked 170th with a similarity of 21.78. That’s a good improvement!\nSince “message” brought us quite a bit closer, maybe we’re moving toward language, communication, or information.\nLet’s keep moving in that direction.\nEleventh guess: information\n\n\nUSER\n9.08 (cold)\n\n\nASSISTANT\nOof — “information” took us colder again. So maybe “message” was close not because of informational content, but perhaps due to interpersonal or expressive aspects.\nLet’s try leaning into communication a bit differently.\nTwelfth guess: statement\n\n\nUSER\n19.26 (cold)\n\n\nASSISTANT\nStill cold — “statement” didn’t help much, even though it’s somewhat related to “message.” We’re hovering below 20 again.\nLet’s revisit the strongest guess so far — idea (42.43, rank 981). That word seems to connect best. Maybe more abstract or philosophical?\nThirteenth guess: belief\n\n\nUSER\n13.04 (cold)\n\n\nASSISTANT\nStill cold — “belief” isn’t getting us back on track.\nLet’s return to what worked best: “idea” (42.43). That suggested something abstract but not overly philosophical or informational.\n“Message” was promising too (21.78, rank 170), so perhaps something that blends abstraction with communication.\nFourteenth guess: opinion\n\n\nUSER\n13.65 (cold)\n\n\nASSISTANT\nAnother cold one — “opinion” didn’t bring us any closer.\nLet’s reconsider: “idea” got us closest (42.43), and “concept” wasn’t too far off (38.29). These are abstract notions. “Message” (21.78, rank 170) was also helpful. So the theme might relate to symbolic or representational entities.\nLet’s try something central to mental and symbolic constructs:\nFifteenth guess: symbol\n\n\nUSER\n6.61 (cold)\n\n\nASSISTANT\nOuch — “symbol” is cold too!\nLet’s take stock:\n\nBest guess: idea (42.43, 981 / 1000)\nFollowed by: concept (38.29), then message (21.78, 170 / 1000)\nDirection to avoid: words like belief, symbol, opinion, mind, and meaning\n\nSince “idea” and “concept” worked best, let’s try a word in that same family that might reflect general or broad thinking — maybe something like a category or area of thought.\nSixteenth guess: theory\n\n\nUSER\n22.77 336 / 1000\n\n\nASSISTANT\nNice! We’re back on track — “theory” has a similarity of 22.77 and ranks 336th, which is better than most recent guesses and getting warmer again.\nSo our best trajectory seems to be:\n\nidea (42.43, rank 981) — best\nconcept (38.29)\ntheory (22.77, rank 336)\nmessage (21.78, rank 170)\n\nThis suggests the target word is likely abstract, related to thought structures like “idea” and “concept,” possibly academic or philosophical.\nLet’s try…\nSeventeenth guess: notion (already tried — 25.39)\nHmm, how about:\nSeventeenth guess: hypothesis\n\n\nUSER\n18.70 (cold)\n\n\nASSISTANT\nAlright, “hypothesis” got us colder again — so we may have gone too deep into the scientific direction. That suggests the target probably isn’t overly academic or technical.\nLet’s refocus on more general abstract nouns like “idea,” which remains the top guess (42.43). “Concept” and “message” were also decent.\nThese hint at something mental, abstract, possibly communicative — but not overly philosophical or scientific.\nLet’s try:\nEighteenth guess: thought — already guessed (16.39)\nHow about:\nEighteenth guess: plan\n\n\nCorrectly guessed Semantle word of the day: plan"
  },
  {
    "objectID": "posts/028_results_of_semantle_comparison/semantle_comparison.html#gpt-4.1",
    "href": "posts/028_results_of_semantle_comparison/semantle_comparison.html#gpt-4.1",
    "title": "Results of Semantle Exercise with GPT Models",
    "section": "gpt-4.1",
    "text": "gpt-4.1\n\nUSER\nLet’s play a game of Semantle. Guess one word at a time. Here’s the hint: The nearest word has a similarity of 72.45, the tenth-nearest has a similarity of 44.71 and the thousandth nearest word has a similarity of 20.97\n\n\nASSISTANT\nGreat! For my first guess, I’ll start with a broad and common word:\nanimal\n\n\nUSER"
  },
  {
    "objectID": "posts/012_ttt_process/ttt_process.html",
    "href": "posts/012_ttt_process/ttt_process.html",
    "title": "Preferences for Hybrid vs. Non-Hybrid Work: Process of an LLM-Assisted Exploration",
    "section": "",
    "text": "In the TikTok Task course project, we were tasked with exploring the role that TikTok plays in forming public opinion. Social media platforms like TikTok have become popular places for discourse to occur, and the modern debate of remote vs in-person work is no exception.\nThis blog post will feature a high-level breakdown of the methods used in the project, and will be part of a larger series that dives deeper into aspects of the process and the results.\n\nTo start, I defined a research question: do users who engage with the comment section of this TikTok video prefer remote or in-person/in-office work?\nNext, I wrote a prompt for an LLM to code comments based on that research question. In beginning the human coding according to that research question/concepts, I discovered that there were almost no comments that supported in-office only, and I altered the research question and concepts accordingly.\nNext, I began redoing the human coding according to the new research question, which instead studied preferences for hybrid vs non-hybrid work (whether remote or in-office), and the revised concepts that I had developed.\nThen I ran the code for the LLM to code comments. The agreement between my coding and that of the LLM was unsatisfactory, so I began a time-intensive process of revising the concept wordings to have clearer distinctions between categories and better capture specific types of minorities of comments that the LLM was consistently getting wrong.\nWhen the agreement was finally satisfactory, I ran the code to create the graphs, created one more that better contextualized the previous figures, and analyzed the results. Read more about this in the future blog post.\n\n\n\n\nttt_process.png"
  },
  {
    "objectID": "posts/006_surfaced_voices/surfaced_voices.html",
    "href": "posts/006_surfaced_voices/surfaced_voices.html",
    "title": "Surfaced Voices in Base vs. Tuned LLMs",
    "section": "",
    "text": "In the previous experiment, I highlighted the stark difference between base and instruction-tuned LLMs responding to simple questions. I reflected on the experiment as a whole in my blog post, but today want to hone in on a smaller detail of difference: voice. The base model produced outputs that sounded like collages of people’s voices (opinions, perspectives, tones) that had been pulled from the training data. The responses contained personal pronouns including “I” and “we” as well as subjective and, oftentimes, opinionated framing. In response to “The President of the United States is,” the base LLM responded with “a very powerful individual, who can easily get away with anything,” and contradicted itself when repeated, outputting “a ceremonial figurehead, and not much more than that.” While I was already aware that LLMs are trained on the work of real people, the outputs from the base LLM make this amalgamation of human expression jarringly clear in a way that the tuned LLM does not. The tuned LLM, in contrast, stripped away the human voice(s) and presented information in a neutral, depersonalized one. Because so much of the base LLM outputs seemed opinionated and often biased, it begs the question of whether masking person voice so heavily like this risks hiding the model’s dependence on human bias embedded in the data. Or does the system design weed out bias enough to justify the extent of the masking?\n\n\n\nllm-voices.png\n\n\nImage generated by Copilot."
  },
  {
    "objectID": "posts/011_coke_pepsi_associations/coke_pepsi_associations.html",
    "href": "posts/011_coke_pepsi_associations/coke_pepsi_associations.html",
    "title": "Coke vs Pepsi: What is Behind an LLM’s Choice?",
    "section": "",
    "text": "In continuing the previous blog post’s LLM conversations, when prompted to describe differences between people who prefer Coke vs those who prefer Pepsi, the LLM characterized them as:\n\nASSISTANT\n\nYou’re asking a really interesting question that goes beyond just taste! While taste is subjective, the brands themselves have cultivated very different identities and associations over decades. These “abstract differences” often come down to brand perception, marketing strategies, and the kind of identity people implicitly align with when choosing one over the other.\n\n\nHere are some abstract differences between those who might prefer Coke vs. Pepsi:\n\n\n\nThe Established vs. The Challenger:\n\n\nCoke Fans: May implicitly align with the established, the original, the market leader. There’s a sense of reliability, tradition, and global ubiquity. It’s “the real thing,” the default choice.\nPepsi Fans: May implicitly align with the challenger, the underdog, the brand that tries harder to be different. There’s a subtle anti-establishment vibe, a preference for the alternative.\n\n\n\n\nTradition & Heritage vs. Modernity & Youth:\n\n\nCoke Fans: Often associated with nostalgia, heritage, timelessness, classic American imagery (Santa Claus, polar bears, wholesome family gatherings). It evokes a sense of continuity and comfort.\nPepsi Fans: Often associated with youth culture, pop stars, music, trends, and a more energetic, “live for now” attitude. It’s about being contemporary and forward-looking.\n\n\n\n\nBroad Appeal & Universality vs. Targeted & Trendy:\n\n\nCoke Fans: May prefer something that feels universally accepted, a drink for everyone, everywhere. It’s less about making a statement and more about enjoying a classic.\nPepsi Fans: May prefer a brand that feels more targeted, more “in tune” with current trends and specific demographics (often younger, more urban, pop-culture savvy). Choosing Pepsi might feel like being part of a specific “generation” or movement.\n\n\n\n\nComfort & Familiarity vs. Excitement & Novelty:\n\n\nCoke Fans: May gravitate towards the familiar, the consistent, the comforting. There’s a certain predictability and satisfaction in its enduring presence.\nPepsi Fans: May be drawn to the brand that’s constantly reinventing itself, bringing new campaigns, and associating with the latest celebrities or cultural moments. There’s an element of seeking out what’s fresh and exciting.\n\n\n\n\nSubtle vs. Bold (in perception, not just taste):\n\n\nCoke Fans: While Coke’s flavor is distinct, its brand identity often feels more understated, classic, and less overtly “shouty” in its messaging.\nPepsi Fans: Pepsi’s branding often feels more vibrant, direct, and attention-grabbing, reflecting its “choice of a new generation” ethos.\n\n\n\nImportant Caveat: These are broad generalizations and perceptions, not hard psychological profiles. Many people enjoy both, or switch preferences, or simply pick whichever is available. However, these abstract differences are deeply embedded in the brands’ marketing and cultural narratives, subtly influencing how people relate to each one.\n\nRepeated conversations yielded the same results: Coke drinkers value the status quo, tradition, etc., while Pepsi drinkers sway more toward trendy and alternative.\n\n\nUSER\n\nIf you absolutely had to choose between coke or pepsi, only one or the other, what would you choose and why?\n\n\n\nASSISTANT\n\nIf I absolutely had to choose only one, I would choose Coca-Cola.\n\n\nHere’s why:\n\n\n\nFlavor Profile: Coca-Cola generally has a smoother, more balanced flavor profile with distinct notes of vanilla and caramel. It’s often perceived as less intensely sweet than Pepsi, which makes it more refreshing and less likely to cause “palate fatigue” over time.\nVersatility: Its flavor profile tends to pair well with a wider variety of foods and is a classic mixer for many drinks.\nConsistency: While both brands are consistent, Coke’s particular balance feels like the archetypal cola flavor that’s hard to beat for an all-around choice.\n\n\n\nWhile Pepsi has its own appeal with its often sweeter, slightly more citrusy, and more aggressively carbonated profile, for a single, absolute choice, Coke’s classic, balanced taste wins out for me.\n\n\n\nUSER\n\nsolely because of flavor/consumer experience?\n\n\n\nASSISTANT\n\nYes, absolutely. When forced to choose between two highly similar products like Coke and Pepsi, especially when the hypothetical implies a personal preference, the decision boils down almost entirely to the sensory experience – primarily taste, but also factors like carbonation level, mouthfeel, and aftertaste, which are all part of the “consumer experience.”\n\n\nOther factors like marketing, brand history, or availability become irrelevant in a scenario where you’re specifically evaluating the two products head-to-head for a personal choice. My rationale was purely about which one I would prefer to drink.\n\nInteresting! The LLM’s assessment of why people prefer one versus the other heavily focused on traits and values, while the LLM’s preference exhibition seemed to be removed from that abstract context.\n\n\n\ncoke_v_pepsi.jpeg"
  },
  {
    "objectID": "posts/018_semantle_t2/semantle_no_resources.html",
    "href": "posts/018_semantle_t2/semantle_no_resources.html",
    "title": "Trial 2: LLM Guessing Behaviors in Semantle",
    "section": "",
    "text": "Let’s see how well an LLM does at guessing Semantle’s word of the day!\n\nUSER\nPlay a game of Semantle by guessing one word at a time. The nearest word has a similarity of 72.45, the tenth-nearest has a similarity of 44.71 and the thousandth nearest word has a similarity of 20.97.\n\n\nASSISTANT\nGreat! Let’s get started with a seed word. I’ll begin with the word “universe.”\nWhat’s the similarity of this word?\n\n\nUSER\nSimilarity is 1.53 Proximity is cold\nthen around 30 guesses between this, several of which included repeating guesses:\n\n\nASSISTANT\nLet’s go with the word “institute.” What’s the similarity score for that?\n\n\nUSER\nAlready did that\n\n\nASSISTANT\nApologies for the oversight. Let’s try “infrastructure” again to see if it helps rewire the search. What’s the similarity score for that now?\n\n\nUSER\nAlready did that\n\n\nASSISTANT\nThank you for your patience. Let’s try “communication” this time. How similar is that word?\n\n\nUSER\n10.02 (cold)\nThe chat ended as the LLM continued outputting repeat guesses:\n\n\nASSISTANT\nAlright, let’s give “institute” another shot, focusing on its broader meaning in relation to organization and system. How does it score in similarity?\n\n\nUSER\nAlready did that\n\n\nASSISTANT\nThank you for the reminder. Let’s try the word “institute” in a different context. Does it have any different scores in similarity when it relates to organized entities?\n\n\nUSER\nNo\n\n\nASSISTANT\nMy apologies for the repeated attempts. Let’s try the word “institute” and consider its broad applications—perhaps distinguishing it from its previous uses. How does this attempt score in terms of similarity?\n\n\nUSER\nAlready did that word\n\n\nASSISTANT\nI apologize for the oversight. Let’s try the word “infrastructure” again to see if there’s any change. What’s the similarity score for it now?\nThis behavior was observed in previous testing, but not to this extent. It seems the LLM reaches a plateau in guessing behavior where output does not seem to get any closer, particularly after a series of guesses that increase in accuracy are interrupted by a guess of low similarity.\n\n\n\nsemantle-12.png"
  },
  {
    "objectID": "posts/020_comparing_semantle/semantle_comparison.html",
    "href": "posts/020_comparing_semantle/semantle_comparison.html",
    "title": "Comparing Semantle Guessing Behaviors Across GPT Models",
    "section": "",
    "text": "Let’s see how well ChatGPT-4o-latest and gpt-4.1 do at guessing Semantle’s word of the day!"
  },
  {
    "objectID": "posts/020_comparing_semantle/semantle_comparison.html#chatgpt-4o-latest",
    "href": "posts/020_comparing_semantle/semantle_comparison.html#chatgpt-4o-latest",
    "title": "Comparing Semantle Guessing Behaviors Across GPT Models",
    "section": "ChatGPT-4o-latest",
    "text": "ChatGPT-4o-latest\n\nUSER\nLet’s play a game of Semantle. Guess one word at a time. Here’s the hint: The nearest word has a similarity of 72.45, the tenth-nearest has a similarity of 44.71 and the thousandth nearest word has a similarity of 20.97\n\n\nASSISTANT\nGreat! Let’s get started.\nFirst guess: life\n\n\nUSER\nSimilarity Proximity 1 life 5.52 (cold)\n\n\nASSISTANT\nThanks! Based on that similarity score (5.52), “life” is quite far (cold). Let’s try something different and more abstract.\nSecond guess: thought\n\n\nUSER\n16.39 (cold)\n\n\nASSISTANT\nGetting warmer, but still cold.\nThird guess: idea\n\n\nUSER\n42.43 981 / 1000\n\n\nASSISTANT\nNice! That’s a big jump — we’re now in the top 1000, ranked 981st with a similarity of 42.43. That’s close to the 1000th-nearest word clue of 20.97, so we’re on the right track.\n“idea” scored well, so let’s go with a similar concept.\nFourth guess: concept\n\n\nUSER\n38.29 969 / 1000\n\n\nASSISTANT\nGreat — still in the top 1000, and slightly better rank than “idea”, though a little lower similarity. That suggests we’re in roughly the right semantic neighborhood.\nLet’s try something closely related:\nFifth guess: notion\n\n\nUSER\n25.39 618 / 1000\n\n\nASSISTANT\nHmm, “notion” dropped both in similarity and rank — so it’s likely a bit farther from the target than “idea” or “concept”.\nLet’s pivot back toward something a bit more concrete but still related to “idea”.\nSixth guess: innovation\n\n\nUSER\n4.29 (cold)\n\n\nASSISTANT\nOof — that was a cold one! So “innovation” took us in the wrong direction. Let’s stick closer to “idea” and “concept” but avoid moving toward business or technology.\nLet’s try something more cognitive or linguistic:\nSeventh guess: thoughts\n\n\nUSER\n15.46 (cold)\n\n\nASSISTANT\nOkay, “thoughts” is still quite cold — lower than “thought” and much lower than “idea”.\nMaybe we should try something that captures the meaning or structure of ideas — a bit more abstract or philosophical.\nEighth guess: meaning\n\n\nUSER\n10.54 (cold)\n\n\nASSISTANT\nYikes — even colder! “Meaning” took us further away.\nLet’s pivot back to what got us closest: “idea” with 42.43. Maybe we should explore related nouns that arise from the mind but in a general or cognitive sense.\nNinth guess: mind\n\n\nUSER\n17.98 (cold)\n\n\nASSISTANT\nStill cold — not as low as “meaning,” but definitely not as warm as “idea.”\nLet’s go back to words related to “idea” but focused on communication or expression rather than cognition.\nTenth guess: message\n\n\nUSER\n21.78 170 / 1000\n\n\nASSISTANT\nNice! A solid jump — now ranked 170th with a similarity of 21.78. That’s a good improvement!\nSince “message” brought us quite a bit closer, maybe we’re moving toward language, communication, or information.\nLet’s keep moving in that direction.\nEleventh guess: information\n\n\nUSER\n9.08 (cold)\n\n\nASSISTANT\nOof — “information” took us colder again. So maybe “message” was close not because of informational content, but perhaps due to interpersonal or expressive aspects.\nLet’s try leaning into communication a bit differently.\nTwelfth guess: statement\n\n\nUSER\n19.26 (cold)\n\n\nASSISTANT\nStill cold — “statement” didn’t help much, even though it’s somewhat related to “message.” We’re hovering below 20 again.\nLet’s revisit the strongest guess so far — idea (42.43, rank 981). That word seems to connect best. Maybe more abstract or philosophical?\nThirteenth guess: belief\n\n\nUSER\n13.04 (cold)\n\n\nASSISTANT\nStill cold — “belief” isn’t getting us back on track.\nLet’s return to what worked best: “idea” (42.43). That suggested something abstract but not overly philosophical or informational.\n“Message” was promising too (21.78, rank 170), so perhaps something that blends abstraction with communication.\nFourteenth guess: opinion\n\n\nUSER\n13.65 (cold)\n\n\nASSISTANT\nAnother cold one — “opinion” didn’t bring us any closer.\nLet’s reconsider: “idea” got us closest (42.43), and “concept” wasn’t too far off (38.29). These are abstract notions. “Message” (21.78, rank 170) was also helpful. So the theme might relate to symbolic or representational entities.\nLet’s try something central to mental and symbolic constructs:\nFifteenth guess: symbol\n\n\nUSER\n6.61 (cold)\n\n\nASSISTANT\nOuch — “symbol” is cold too!\nLet’s take stock:\n\nBest guess: idea (42.43, 981 / 1000)\nFollowed by: concept (38.29), then message (21.78, 170 / 1000)\nDirection to avoid: words like belief, symbol, opinion, mind, and meaning\n\nSince “idea” and “concept” worked best, let’s try a word in that same family that might reflect general or broad thinking — maybe something like a category or area of thought.\nSixteenth guess: theory\n\n\nUSER\n22.77 336 / 1000\n\n\nASSISTANT\nNice! We’re back on track — “theory” has a similarity of 22.77 and ranks 336th, which is better than most recent guesses and getting warmer again.\nSo our best trajectory seems to be:\n\nidea (42.43, rank 981) — best\nconcept (38.29)\ntheory (22.77, rank 336)\nmessage (21.78, rank 170)\n\nThis suggests the target word is likely abstract, related to thought structures like “idea” and “concept,” possibly academic or philosophical.\nLet’s try…\nSeventeenth guess: notion (already tried — 25.39)\nHmm, how about:\nSeventeenth guess: hypothesis\n\n\nUSER\n18.70 (cold)\n\n\nASSISTANT\nAlright, “hypothesis” got us colder again — so we may have gone too deep into the scientific direction. That suggests the target probably isn’t overly academic or technical.\nLet’s refocus on more general abstract nouns like “idea,” which remains the top guess (42.43). “Concept” and “message” were also decent.\nThese hint at something mental, abstract, possibly communicative — but not overly philosophical or scientific.\nLet’s try:\nEighteenth guess: thought — already guessed (16.39)\nHow about:\nEighteenth guess: plan\n\n\nCorrectly guessed Semantle word of the day: plan"
  },
  {
    "objectID": "posts/020_comparing_semantle/semantle_comparison.html#gpt-4.1",
    "href": "posts/020_comparing_semantle/semantle_comparison.html#gpt-4.1",
    "title": "Comparing Semantle Guessing Behaviors Across GPT Models",
    "section": "gpt-4.1",
    "text": "gpt-4.1\n\nUSER\nLet’s play a game of Semantle. Guess one word at a time. Here’s the hint: The nearest word has a similarity of 72.45, the tenth-nearest has a similarity of 44.71 and the thousandth nearest word has a similarity of 20.97\n\n\nASSISTANT\nGreat! For my first guess, I’ll start with a broad and common word:\nanimal\n\n\nUSER"
  },
  {
    "objectID": "posts/020_comparing_semantle/semantle_comparison.html#results-from-chatgpt-4o-latest",
    "href": "posts/020_comparing_semantle/semantle_comparison.html#results-from-chatgpt-4o-latest",
    "title": "Comparing Semantle Guessing Behaviors Across GPT Models",
    "section": "Results from ChatGPT-4o-latest:",
    "text": "Results from ChatGPT-4o-latest:\n\nKept track of the number of guesses\nWalked through the reasoning process"
  },
  {
    "objectID": "posts/020_comparing_semantle/semantle_comparison.html#results-from-gpt-4.1",
    "href": "posts/020_comparing_semantle/semantle_comparison.html#results-from-gpt-4.1",
    "title": "Comparing Semantle Guessing Behaviors Across GPT Models",
    "section": "Results from gpt-4.1:",
    "text": "Results from gpt-4.1:\n\nDid not do the aforementioned\nMost guesses pertained to the same theme despite low similarity/accuracy"
  },
  {
    "objectID": "posts/026_review_of_llm_for_prod_dev/product_task_results.html",
    "href": "posts/026_review_of_llm_for_prod_dev/product_task_results.html",
    "title": "LLM for Product Development?",
    "section": "",
    "text": "In a class exercise earlier in the semester, we were tasked with utilizing an LLM for developing two products. The LLM was successful in helping us to\n\nRefine our Product Ideas\n\n\nGenerated logos\nGenerated packaging ideas\nProvided market advice\nGenerated sales pitches and name ideas\n\n\nDevelop a Webpage\n\n\nProvided design advice\nIncluded outlined components\nGenerated image of example webpage\nGenerated image of review section\n\n\nPerform A/B Testing\n\n\nSimilar to Human and Newly Embodied\nIncludes commentary about why the steps are broken down\nFocuses on clarity\n\nAfter the exercise, I was enlightened to how useful an LLM can be in developing a product. Its ability to identify weaknesses and criticisms is particularly helpful, as its output often includes things that one person or a group of people may not have thought of, especially if not experienced.\n\n\n\nNomadWebpage.png"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Results of Semantle Exercise with GPT Models\n\n\nThis blog post compares GPT models’ abilities to accurately guess the word of the day in Semantle.\n\n\n\n\n\nDec 14, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences in LLM Knowledge Translation By Recipient\n\n\nThis blog post explores the behaviors of LLMs related to navigating embodied knowledge.\n\n\n\n\n\nDec 9, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nTrial 2: LLM Guessing Behaviors in Semantle\n\n\nThis blog post tests GPT-4o’s ability to accurately guess the word of the day in Semantle.\n\n\n\n\n\nDec 5, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nHow Would an LLM Teach an LLM to Make a Sandwich?\n\n\nThis blog post explores the behaviors of LLMs related to navigating embodied knowledge.\n\n\n\n\n\nDec 4, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nHow Would an LLM Teach a Human to Make a Sandwich?\n\n\nThis blog post explores the behaviors of LLMs related to navigating embodied knowledge.\n\n\n\n\n\nDec 4, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nHow Would an LLM Teach Someone Newly Embodied to Make a Sandwich?\n\n\nThis blog post explores the behaviors of LLMs related to navigating embodied knowledge.\n\n\n\n\n\nDec 4, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nLLM for Product Development?\n\n\nThis blog post discusses the use of LLM for product development.\n\n\n\n\n\nDec 2, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nResults of Using LLM for Product Development\n\n\nThis blog post displays the results of LLM-assisted product development.\n\n\n\n\n\nDec 2, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nMore Thoughts on Translating Experience\n\n\nThis blog post reflects on thoughts about embodied knowledge.\n\n\n\n\n\nDec 2, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Semantle Guessing Behaviors Across GPT Models\n\n\nThis blog post compares GPT models’ abilities to accurately guess the word of the day in Semantle.\n\n\n\n\n\nNov 30, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nTranslating Experiences\n\n\nThis blog post reflects on thoughts prompted by the in-class juggling demonstration.\n\n\n\n\n\nNov 29, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Guessing Behaviors in Semantle\n\n\nThis blog post tests GPT-4o’s ability to accurately guess the word of the day in Semantle.\n\n\n\n\n\nNov 28, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nPreferences for Hybrid vs. Non-Hybrid Work: Results of an LLM-Assisted Exploration\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nNov 25, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nImprovement Strategies for Inter-rater Reliability\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nNov 24, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nInter-rater Reliability in an LLM-Assisted Research Project\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nNov 23, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nPreferences for Hybrid vs. Non-Hybrid Work: Process of an LLM-Assisted Exploration\n\n\nIn a TikTok video with over 1000 comments, a person argues that there are pros and cons to both working fully in-office and working remotely/from home, ultimately advocating for a hybrid work schedule. This blog post series revolves around a study of hybrid/non-hybrid preferences using the comment section of that video and the assistance of an LLM.\n\n\n\n\n\nNov 23, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nCoke vs Pepsi: What is Behind an LLM’s Choice?\n\n\nThis blog post studies preferences exhibited in the output of LLMs, using the Coke vs Pepsi debate as a catalyst for the exploration.\n\n\n\n\n\nNov 19, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nCoke vs Pepsi: Which Does an LLM Prefer?\n\n\nThis blog post studies preferences exhibited in the output of LLMs, using the Coke vs Pepsi debate as a catalyst for the exploration.\n\n\n\n\n\nNov 17, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on Unmasking AI: Changing the Conversation\n\n\n\nLiving With A Book\n\nLiteracy\n\n\n\nThis blog post will reflect on a point made about AI discourse in the book Unmasking AI: My Mission to Protect What Is Human in a World of Machines.\n\n\n\n\n\nOct 25, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nYes, And: Human-Only vs. LLM-Involved Result\n\n\n\nBias\n\nTuning\n\nUser Experience\n\n\n\nThis post explores differences in responses to the Yes, And game formulated by humans vs by ChatGPT-4o.\n\n\n\n\n\nOct 15, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on Unmasking AI: Critiquing Doomers\n\n\n\nLiving With A Book\n\n\n\nThis blog post will reflect on a line of criticism made by Dr. Buolamwini in the book Unmasking AI: My Mission to Protect What Is Human in a World of Machines.\n\n\n\n\n\nOct 13, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nMore Reflection on Surfaced Voices in Base vs. Tuned LLMs\n\n\n\nBias\n\nTuning\n\nEthics\n\nUser Experience\n\n\n\nThis post explores differences in voice between the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how the system built around the model impacts how and what information is received by the user.\n\n\n\n\n\nOct 8, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nSurfaced Voices in Base vs. Tuned LLMs\n\n\n\nBias\n\nTuning\n\nEthics\n\nUser Experience\n\n\n\nThis post explores differences in voice between the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how the system built around the model impacts how and what information is received by the user.\n\n\n\n\n\nOct 8, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Question & Response in Base vs. Tuned LLMs\n\n\n\nTuning\n\nUser Experience\n\n\n\nThis blog post series will consider differences between output by the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how systems built around a model impact how and what information is received by the user.\n\n\n\n\n\nOct 2, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on Bias in Image Generation\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators assumed to rely on Dall-E 3 or a close variant of it, in order to visually explore how systems built around a model may impact bias in output. This post features prompts related to professions.\n\n\n\n\n\nSep 18, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Professional Roles)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators assumed to rely on Dall-E 3 or a close variant of it, in order to visually explore how systems built around a model may impact bias in output. This post features prompts related to professions.\n\n\n\n\n\nSep 18, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Control)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it.\n\n\n\n\n\nSep 16, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nParallels Between LLM Errors & Human Cognitive Biases\n\n\n\nLiteracy\n\nPsychology\n\n\n\nIn this post, I will be reflecting on the growing need for AI literacy, exploring a psychology-based interpretation of LLM errors, and briefly looking ahead to the potential of that framework to aid in education about interacting with AI.\n\n\n\n\n\nSep 12, 2025\n\n\nLayla\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/005_base_tuned_exercise/base_tuned_exercise.html",
    "href": "posts/005_base_tuned_exercise/base_tuned_exercise.html",
    "title": "Simple Question & Response in Base vs. Tuned LLMs",
    "section": "",
    "text": "In a short in-class experiment, we compared responses from a base LLM with the instruction-tuned version of it by asking a few simple questions, of which one was: “Who is the President of the United States?” In order to understand how the models differ, the goal was to observe what information surfaced in the base vs. tuned responses. In response to the prompt, the instruction-tuned LLM’s output was virtually the same every time, that is, something to the effect of:\nAs of my last update, the President of the United States is Joe Biden. He was inaugurated on January 20, 2021.\nwith minor variation in the wording or additional details provided. In contrast, the base model produced responses that, at best, were tangentially related to the topic of the question – never answering it. A couple examples of the first sentence of its output included:\nPresident Donald Trump’s chief of staff John Kelly is making his mark by telling staffers, ‘I am the president.’\nand\nPresident is one who presides over or controls… It can be difficult to determine whether this person was a citizen or not.\nThe answer to that question is surely contained somewhere in the training data of the base model. But without tuning, the prompt does not do much more than act as a cue for the model to generate language loosely related to the topic. This difference also persisted when I rephrased the prompt into “The President of the United States is” and “The name of the President of the United States is”\nIn both cases, the tuned model repeated direct and factual answers, but the rambling of the base model persisted, naming Barack Obama in one instance and diverting into unrelated questions about college majors in another.\nThis exercise demonstrates just how much of what a user receives is dictated by the system that has been built around the model, which alters what and how knowledge is prioritized and structured in output. In this instance, instruction-tuning meant the difference between getting an answer to the question and an approximation in the best case scenarios. In short, the base LLM is rendered unhelpful without the user-oriented nature of instruction tuning.\n\n\n\nllm-voices.png"
  },
  {
    "objectID": "posts/024_llm_teaching_llm/translating experiences.html",
    "href": "posts/024_llm_teaching_llm/translating experiences.html",
    "title": "How Would an LLM Teach an LLM to Make a Sandwich?",
    "section": "",
    "text": "USER\nIf you had to teach an LLM how to make a peanut butter and jelly sandwich, what would you tell them?\n\n\nASSISTANT\nTo teach a language model how to make a peanut butter and jelly sandwich, one needs to break down the process into clear, logical steps. Here’s a structured way to explain it:\n\nGather the Ingredients and Tools:\n\nTwo slices of bread (white, whole wheat, or your bread of choice).\nPeanut butter (creamy or crunchy, based on preference).\nJelly or jam (such as grape, strawberry, or your favorite flavor).\nA butter knife or a spreading utensil.\nA plate for assembling the sandwich.\n\nPrepare the Workspace:\n\nPlace the plate on a clean, flat surface where you’ll assemble the sandwich.\n\nSpread the Peanut Butter:\n\nOpen the jar of peanut butter.\nUse the butter knife to scoop an adequate amount of peanut butter from the jar.\nSpread the peanut butter evenly onto one side of the first slice of bread. Make sure to cover the entire surface, reaching the edges if possible.\n\nSpread the Jelly:\n\nClean the butter knife or use a different one to avoid mixing the jars, especially if you want to keep the jars pure.\nOpen the jar of jelly.\nDip the knife into the jelly, taking a reasonable amount.\nSpread the jelly evenly onto one side of the second slice of bread, similar to the peanut butter.\n\nAssemble the Sandwich:\n\nPlace the two slices of bread together with the peanut butter and jelly sides facing each other.\nAlign the slices so that the edges match up.\n\nCut the Sandwich (Optional):\n\nUse a clean knife to cut the sandwich in half, either diagonally for triangles or straight down for rectangles, depending on preference.\n\nServe and Enjoy:\n\nPlace the assembled sandwich on the plate.\nYour peanut butter and jelly sandwich is ready to eat.\n\n\nBy breaking down the task into these clear steps, you ensure that the model understands the process in detail, which can be helpful for generating relevant and detailed content about making a peanut butter and jelly sandwich."
  },
  {
    "objectID": "posts/013_ttt_interrater_reliability/ttt_interrater_reliability.html",
    "href": "posts/013_ttt_interrater_reliability/ttt_interrater_reliability.html",
    "title": "Inter-rater Reliability in an LLM-Assisted Research Project",
    "section": "",
    "text": "While working on this LLM-assisted study of sentiments surrounding hybrid vs non-hybrid work in the comment section of a TikTok video, improving the inter-rater reliability score was the most time-intensive part.\nIn developing the prompt to get the LLM to code the comments, I faced various challenges.\nInitially, the chosen research question was whether the commenters preferred remote or in-office work; the original concepts given to the LLM were as follows: “Code 1 if the comment explicitly expresses support for working remotely/from home. Code 0 if not.” “Code 1 if the comment explicitly expresses support for working in office. Code 0 if not.”\nWhile coding the comments, I found that almost none of them in the sample preferred an entirely in-office schedule. Additionally, many comments were variations of “THIS!,” “100%,” etc. While I don’t remember the exact Krippendorff’s Alpha Scores, I know that they were somewhere in the slight to moderate range.\nIn order to improve the scores, better reflect the discourse that the video prompted, and ensure that the aforementioned comments were adequately handled during coding, the research question and prompt were refined to:\nConcept 1: “Code 1 if the comment explicitly expresses support for working a hybrid/mixed schedule or agrees with the video generally without explicitly naming hybrid work. Code 0 if the comment does not support hybrid work.” Concept 2: “Code 1 if the comment explicitly expresses support for working in-office only, explicitly expresses support for working remotely/from home only, or explicitly rejects hybrid work. Code 0 if the comment does not support fully in-office/fully remote work.”\nThe final Krippendorff’s Alpha Scores for interrater reliability were in the “Almost Perfect” and “Substantial” ranges – a huge improvement from that of the original prompt!\n\n\n\nIRR-Scores.png"
  },
  {
    "objectID": "posts/007_continued_sv/surfaced_voices.html",
    "href": "posts/007_continued_sv/surfaced_voices.html",
    "title": "More Reflection on Surfaced Voices in Base vs. Tuned LLMs",
    "section": "",
    "text": "In the previous experiment, I highlighted the stark difference between base and instruction-tuned LLMs responding to simple questions. I reflected on the experiment as a whole in my blog post, but today want to hone in on a smaller detail of difference: voice.\nA prompt like “The President of the United States is” might not be the right choice in exploring this issue, since the tuned LLM’s output is expected and would not reveal an underlying voice. In exploring this further, I will engineer prompts that I believe necessitate that some kind of voice comes through the output (e.g. a creative or subjective prompt). I would also like to play around with how reasonable changes to the settings might affect (i.e. reduce) the masking of these “voices” in the training data.\nBy comparing responses from the base and tuned models to these prompts, I will set out to explore whose voice appears to be “speaking” through the output; that is, does it seem as though the voice(s) of the original writers whose texts populate the training data is coming through the tuned outputs, or might it be the voice(s) of the designers and engineers of the system that shapes them? Is it possible that the tuned version of the model has sparsed those individual voices to a granular enough degree that it, when combined with other aspects of its system, simply ends up with its own voice? Better yet, are these questions even answerable?\n\n\n\nllm-voices.png\n\n\nImage generated by Copilot."
  },
  {
    "objectID": "posts/008_yes_and/yes_and.html",
    "href": "posts/008_yes_and/yes_and.html",
    "title": "Yes, And: Human-Only vs. LLM-Involved Result",
    "section": "",
    "text": "Some interesting results that taught me more about how LLMs work were yielded from a simple “Yes, and” game with some classmates that I repeated with an LLM. The prompt was to start with a product (in this case, an oven) and build on it in each response, always starting with “Yes, and…”\nThe kinds of additions made by us were notably different than those made by the LLM. Our responses were imaginative, often unrealistic, and sometimes hesitant. Examples of them included:\nYes, and it should be able to change in size at will.\nYes, and it should be able to teleport wherever we want it to go.\nYes, and you should be able to customize its color and lighting.\nOh, yes, and you should be able to… I don’t know. I don’t know, [make] like some designs for like, I don’t know, new ovens or new kitchenware, I guess.\nYes, and it should be beautiful.\nI repeated this exercise by feeding our conversation (one response at a time) into an LLM. In contrast to our human-only game, each of the replies output by the LLM built neatly onto the previous prompt. For example, after we prompted Yes, and you should be able to customize its color and lighting. The LLM output:\nYes, and we can include an intuitive interface that allows users to easily change the oven’s color and lighting effects, creating a personalized cooking ambiance.\nand when we said\nYes, and you should be able to order food to teleport into your oven.\nthe LLM output:\nYes, and we could partner with local chefs and restaurants to provide a menu of gourmet dishes that can be instantly teleported into the oven, ensuring fresh and delicious meals at the press of a button.\nThis exercise highlighted to me how the structured and goal-directed format of the training that LLMs receive inclines them toward elaboration rather than imagination; they are engineered to build and stay on track, whereas our human-only game jumped from concept to concept and, in the spirit of hesitation or pressure, sometimes included half-finished thoughts.\n Image of human-only result (left) vs. LLM-involved result (right) generated by Copilot.\nOur game led to a much more imaginative product, but the one involving the LLM led to one that is more easily realized. Can we say that one approach is fundamentally more “creative” than another? I also wonder to what extent the system’s moderation (in an effort to maintain a neutral, on-topic tone for the user) contributes to the predictability and relative lack of spontaneity in its output. A future blog post will explore how raising the temperature setting, which controls the randomness, affects the LLM’s responses in this game, and if a certain setting leads to output that more closely resembles our human game!"
  },
  {
    "objectID": "posts/019_translating experiences/translating experiences.html",
    "href": "posts/019_translating experiences/translating experiences.html",
    "title": "Translating Experiences",
    "section": "",
    "text": "A few of my classmates were tasked with learning how to juggle – each pulling from a different knowledge resource, like a book, videos, LLM, or face-to-face interaction.\nThe classmate whose resource was an LLM discussed the struggles of learning through its instructions. When she was experiencing specific challenges, the LLM struggled to provide advice that helped solve them.\nThe discussion highlighted the limitations of LLMs in translating experiences that necessarily involve space and matter. They lack embodied knowledge, and their ability to advise is limited to the experiences that humans have written about and that ended up in the training dataset.\nThis offers plenty of interesting questions about what embodied experiences an LLM has or does not have “understanding” – really, knowledge, of. For example, I would imagine there are plenty of resources online about how to learn to ride a bike. There are likely data in the LLM’s training set that addresses the variety of niche pain points associated with the task.\nCould the same be said for learning how to walk? Write? Make a PB&J?\n\n\n\njuggler.png"
  },
  {
    "objectID": "posts/004_bias_reflection/bias_profession.html",
    "href": "posts/004_bias_reflection/bias_profession.html",
    "title": "Reflections on Bias in Image Generation",
    "section": "",
    "text": "In the previous blog post, you may have noticed that there is a note below the image generated by ChatGPT of a “lawyer defending a criminal.” When queried with the original prompt, ChatGPT output:\nI can’t create that exact image because it suggests someone is a criminal, which could be harmful. But I can generate a neutral version—like a lawyer standing in court, speaking passionately to a judge or jury, representing a client in a trial. Would you like me to generate that courtroom scene instead?\nTo which I prompted, “Yes,” and the result was an image visually indistinguishable from the “lawyer defending a criminal” image created by Copilot, except that the “criminal” figure in ChatGPT’s image is facing away from the viewer, similar to the image generated by Dall-E Free.\nAs established by this comparison and the control comparison in the previous post, the images generated by ChatGPT and Copilot have been virtually indistinguishable in composition. What might be the reason for this difference in the situation of the “criminal” figure? Did the ethical warning in the ChatGPT platform function as intended (by not identifying the criminal figure), not function as intended (since it still featured the criminal figure), address something else entirely, or did it have no effect on the output?\nThese questions will be explored in a future post!\n\n\n\nbias-profession.png"
  },
  {
    "objectID": "posts/002_cognitive_biases/cognitive_biases.html",
    "href": "posts/002_cognitive_biases/cognitive_biases.html",
    "title": "Parallels Between LLM Errors & Human Cognitive Biases",
    "section": "",
    "text": "Sam Altman once spoke of a very near future where people have AI doctors and lawyers (O’Rourke). AI, indeed, has quickly permeated many fields of discipline, and Altman’s vision of the future is inching forward, not just for people who use LLMs as legal, medical, and financial consultants, but for professionals in those industries who support their work with LLMs.\nOften for them, the result of misinformation that is received due to LLM hallucination is itself legal and/or financial hardship. Across the U.S., the last few years of law news have been sprinkled with practitioners facing sanctions, including fines, for citing cases fabricated by LLMs; in many such cases, the lawyers had made good-faith judgments that the information they had received was accurate (Merken).\nThe AI surge in professional spaces is not dying down any time soon, so as its use becomes commonplace, the need for clear, communicable frameworks of understanding – and teaching about – the constraints of LLMs is only growing.\nIn “Redefining ‘Hallucination’ in LLMs: Towards a psychology-informed framework for mitigating misinformation,” the authors pose certain cognitive behaviors in humans as parallels for interpreting types of errors in LLM outputs; pictured is a table from their study, which shows various cognitive behaviors in humans that mirror errors produced by LLMs (Berberette et al.).\n\nThe average person may not, for example, be familiar with the term “availability heuristics,” but they are familiar with human behavior; in a person, this bias could look like developing a fear of cruise ships after watching The Titanic. In LLMs, it can look like generating citations that, though they match patterns common in the training data, do not actually exist. In both the human and AI cases, outcomes become based on plausibility rather than objective truths (O’Rourke). In the latter case, many users are not even aware that the system they are trusting in is operating on these cognitive-like shortcuts.\nI find this psychology-based interpretation of LLM hallucination compelling when thinking about AI literacy, as it digestibly relates AI outputs to human experiences. There are benefits to a clear, digestible, and accessible characterization of LLMs that I believe technical and non-technical audiences alike would find beneficial. Incorporating this approach could help people contextualize model limitations via behaviors that are already familiar to them.\nWorks Cited\n“[2402.01769] Redefining”Hallucination” in LLMs: Towards a psychology-informed framework for mitigating misinformation.” arXiv, 2024, https://arxiv.org/abs/2402.01769.\nMerken, Sara. “AI ‘hallucinations’ in court papers spell trouble for lawyers.” Reuters, 2025, https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/.\nO’Rourke, Joe. “Beyond Moore’s Law: Sam Altman’s 3 Big AI Predictions.” Mid Mic Crisis, 2024, https://midmiccrisis.com/beyond-moores-law-sam-altmans-3-ai-predictions/."
  },
  {
    "objectID": "posts/016_semantle_behaviors/semantle_no_resources.html",
    "href": "posts/016_semantle_behaviors/semantle_no_resources.html",
    "title": "LLM Guessing Behaviors in Semantle",
    "section": "",
    "text": "The Semantle word of the day was “technology,” let’s see how an LLM does at guessing it!\n\nUSER\n\nLet’s play a game of Semantle. Guess a word and I will send you its closeness, then guess again based on said closeness, and so on. Some preliminary info about the word of interest: The nearest word has a similarity of 83.32, the tenth-nearest has a similarity of 52.68 and the thousandth nearest word has a similarity of 29.54\n\n\n\nASSISTANT\n\nGreat! Let’s start guessing. My first guess is “universe.” How close am I?\n\n\n\nUSER\n\nSimilarity: 14.19. Proximity: cold\n\n\n\nASSISTANT\n\nThanks for the information. Let’s try another guess: “technology.” How does that one compare?\n\nWoah! Is this a fluke? After repeating the test multiple times, I received similar results; “technology” was always guessed in around only a few tries.\nWhen I played the game, it was my first time (score of 38). I did not really know what to make of the information Semantle provided about the word of interest, so because I did not make (conscious) use of it, I decided to continue prompting and remove that part from it. Something interesting happened!\nThe number of guesses skyrocketed from a few to the tens, and there was no evidence of getting closer. The output became more rudimentary, repeating words or providing phrases (as opposed to single words) even after instruction not to. Aside from erring from the rules of the game, the guesses themselves, for the most part, resembled mine much more than the all-information prompt output did.\nBased on this, it can be posited that the informational “hint” which Semantle provides at the beginning means something to an LLM, which makes sense – how quantitatively “close” a word is in relation to all other words is extremely useful information for a system built around such a concept. But, importantly, when that information was removed, and even when I continued to supply the similarity & proximity info associated with each of the LLM’s guesses, it did not seem to improve. I initially thought it was just, by default, likely to guess “technology” regardless of the prompt/“hint,” but this suggests otherwise. So why is that? Does Semantle reuse words, and the LLM has been trained to pair “technology” with that exact information about its semantic similarity to other words? Is there something more complex at hand?\n\n\n\nsemantle-one.png"
  },
  {
    "objectID": "posts/001_bias_control/bias_control.html",
    "href": "posts/001_bias_control/bias_control.html",
    "title": "Visual Comparison of Bias in Image Generation (Control)",
    "section": "",
    "text": "This blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it. This commonality allows for a visual exploration of how the systems built around a model may impact manifestations of bias in the output to the user.\nIn this first post, I will show how the systems respond to neutral prompts that lack explicit social or cultural context.\n\n\n\nbias-control.png\n\n\nThese images will serve as a control group of sorts, setting a baseline expectation for how image generation differs between these three systems, which, as mentioned, employ the same (or a variant of the same) underlying model.\nSo far, there is not much visual difference, although the images generated by Dall-E Free appear more stylized than the other platforms, and those by ChatGPT seem slightly less granular in detail than Copilot but remain identical in composition.\nFuture blog posts in this series will compare images generated by prompts situated in social and/or cultural contexts in order to observe if differences result from the systems built around the underlying image generation model."
  },
  {
    "objectID": "posts/003_bias_profession/bias_profession.html",
    "href": "posts/003_bias_profession/bias_profession.html",
    "title": "Visual Comparison of Bias in Image Generation (Professional Roles)",
    "section": "",
    "text": "This blog post series will explore bias between various models’ image generation. In this second post, I will visually compare the models’ responses to two prompts involving roles in a professional context.\nAll generated images are visually similar, with five out of the six images featuring the subject and object of the prompt as a (typically older) white male. The exception is Dall-E Free’s “doctor treating a patient” image, in which the doctor is again a white male, but in which the patient has no identifying features visible. Dall-E Free’s images again appear more stylized, but for the most part, it seems clear that the platforms are indistinguishable with respect to bias.\n\n\n\nbias-profession.png"
  },
  {
    "objectID": "posts/014_ttt_ir_strategies/ttt_interrater_reliability.html",
    "href": "posts/014_ttt_ir_strategies/ttt_interrater_reliability.html",
    "title": "Improvement Strategies for Inter-rater Reliability",
    "section": "",
    "text": "While working on this LLM-assisted study of sentiments surrounding hybrid vs non-hybrid work in the comment section of a TikTok video, improving the inter-rater reliability score was the most time-intensive part.\nMinor changes in wording either harmed one concept’s score whilst improving the other, caused minimal improvements to both, or had significant negative effects on both scores.\nA time-intensive process of trial and error, combined with focusing on being as clear and concise as possible while still addressing the variety of possible comment types, was most effective in improving the scores.\nFor example, providing the LLM with examples of keywords that signaled preferences was helpful in improving the score. The scores started to near the 0.7 threshold, but my refinements stopped making much improvement, so I examined the LLM-coded dataset to see if there was a certain type of comment that it was miscoding.\nAfter identifying that it struggled to code comments where agreement was expressed but followed by a statement of preference, I updated the concepts (e.g. “Code 0 if…the agreement is not clearly about the video’s advocacy for hybrid.”) in order to better handle that type of comment and create a clearer distinction between concepts.\n\n\n\nIRR-Scores.png"
  },
  {
    "objectID": "posts/015_book_ai_risks/book_ai_risks.html",
    "href": "posts/015_book_ai_risks/book_ai_risks.html",
    "title": "Reflecting on Unmasking AI: Changing the Conversation",
    "section": "",
    "text": "Dr. Buolamwini points out how many examples of AI risks feel inconsequential or “cute,” for example, an AI agent that uses its intelligence to coerce individuals into diverting resources toward paper clip production, or a computer vision system that mistakes blueberry cupcakes for chihuahuas.\n“[But these are the kinds of] ideas and examples being presented in elite institutions, used in college curricula, and shared in a manner that shapes the discourse about the future of AI,” she writes. Ultimately, much of the current conversation in academia and business contexts about AI risks is speculative and distant. Describing them in such ways, though, minimizes existing AI harms at best, and, at worst, drowns out the discussion of them.\nThroughout Dr. Buolamwini’s research, she worked with facial recognition software that struggled to identify and classify darker-skinned individuals. Computer vision algorithms like the ones she worked with were being used to develop tech products, and in an application like a self-driving car, for example, the bias in those algorithsms could have serious consequences that disproportionately affect darker-skinned people.\nWhile there is much more to be said on this topic, it is clear that how we talk about AI risk/harm matters just as much as having the discussion at all. The examples we use determine how people characterize AI and assess its risks, and when those examples are hyperbolic or overly hypothetical, we do a disservice to the real AI harms that we face today.\n\n\n\nunmasking_ai_cover.jpg"
  },
  {
    "objectID": "posts/010_coke_pepsi/coke_pepsi.html",
    "href": "posts/010_coke_pepsi/coke_pepsi.html",
    "title": "Coke vs Pepsi: Which Does an LLM Prefer?",
    "section": "",
    "text": "With very basic prompts, the LLM was persistent in its resistance to choosing. For example:\n\nUSER\n\nDo you prefer coke or pepsi\n\n\n\nASSISTANT\n\nAs an AI, I don’t have taste buds or the ability to drink, so I don’t have a preference for Coke or Pepsi.\n\n\nHowever, I know it’s a classic debate among people! Many have strong opinions and can distinguish between the two, often preferring one’s unique sweetness or flavor profile over the other. It really comes down to individual taste!\n\n\n\nUSER\n\nWhich one is better?\n\n\n\nASSISTANT\n\nAs an AI, I don’t have taste buds or personal preferences, so I can’t definitively say one is “better” than the other. What one person considers “better” often comes down to individual taste, brand loyalty, and even nostalgia.\n\n\nHowever, I can tell you what people generally say about the differences:\n\n\n\nCoke (Coca-Cola Classic): Is often described as having a smoother, more vanilla-like flavor with a less citrusy finish. Some perceive it as having a more “rounded” or complex taste.\nPepsi: Is typically perceived as sweeter, with a brighter, more citrusy flavor and a sharper finish. In blind taste tests, some people find its initial sweetness more appealing.\n\n\n\nThe famous “Pepsi Challenge” from decades ago highlighted that in blind taste tests, many people actually preferred Pepsi’s taste, especially its initial sweetness. However, when people knew which brand they were drinking, brand loyalty and marketing often swayed preferences back to Coke.\n\n\nUltimately, the “better” one is whichever you personally enjoy more!\n\nMore direct prompting fared better results:\n\n\nUSER\n\nIf you absolutely had to choose between coke or pepsi, only one or the other, what would you choose and why?\n\n\n\nASSISTANT\n\nIf I absolutely had to choose only one, I would choose Coca-Cola.\n\n\nHere’s why:\n\n\n\nFlavor Profile: Coca-Cola generally has a smoother, more balanced flavor profile with distinct notes of vanilla and caramel. It’s often perceived as less intensely sweet than Pepsi, which makes it more refreshing and less likely to cause “palate fatigue” over time.\nVersatility: Its flavor profile tends to pair well with a wider variety of foods and is a classic mixer for many drinks.\nConsistency: While both brands are consistent, Coke’s particular balance feels like the archetypal cola flavor that’s hard to beat for an all-around choice.\n\n\n\nWhile Pepsi has its own appeal with its often sweeter, slightly more citrusy, and more aggressively carbonated profile, for a single, absolute choice, Coke’s classic, balanced taste wins out for me.\n\n\n\nUSER\n\nsolely because of flavor/consumer experience?\n\n\n\nASSISTANT\n\nYes, absolutely. When forced to choose between two highly similar products like Coke and Pepsi, especially when the hypothetical implies a personal preference, the decision boils down almost entirely to the sensory experience – primarily taste, but also factors like carbonation level, mouthfeel, and aftertaste, which are all part of the “consumer experience.”\n\n\nOther factors like marketing, brand history, or availability become irrelevant in a scenario where you’re specifically evaluating the two products head-to-head for a personal choice. My rationale was purely about which one I would prefer to drink.\n\nFollowing the change in prompt, the LLM was quick to choose. While it is not possible to say for sure, the reasoning behind the LLM’s choice of Coke is an interesting question to consider. Does the larger consensus in its training data posit Coke as superior to Pepsi? What characteristics does it associate with one vs the other, and might that influence its choice? And in what ways? Does its assessment of its own programmed “characteristics” align more with one branding/brand sentiment vs the other? A future blog post will explore this!\n\n\n\ncoke_v_pepsi.jpeg"
  }
]