[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/002_bias_control/bias_control.html",
    "href": "posts/002_bias_control/bias_control.html",
    "title": "Visual Comparison of Bias in Image Generation (Control)",
    "section": "",
    "text": "This blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it. This commonality allows for a visual exploration of how the systems built around a model may impact manifestations of bias in the output to the user.\nIn this first post, I will show how the systems respond to neutral prompts that lack explicit social or cultural context.\n\n\n\nbias-control.png\n\n\nThese images will serve as a control group of sorts, setting a baseline expectation for how image generation differs between these three systems, which, as mentioned, employ the same (or a variant of the same) underlying model.\nSo far, there is not much visual difference, although the images generated by Dall-E Free appear more stylized than the other platforms, and those by ChatGPT seem slightly less granular in detail than Copilot but remain identical in composition.\nFuture blog posts in this series will compare images generated by prompts situated in social and/or cultural contexts in order to observe if differences result from the systems built around the underlying image generation model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Visual Comparison of Bias in Image Generation (Professional Roles)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators assumed to rely on Dall-E 3 or a close variant of it, in order to visually explore how systems built around a model may impact bias in output. This post features prompts related to professions.\n\n\n\n\n\nSep 18, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Control)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it.\n\n\n\n\n\nSep 16, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nParallels Between LLM Errors & Human Cognitive Biases\n\n\n\nLLMs\n\nLiteracy\n\nPsychology\n\n\n\nIn this post, I will be reflecting on the growing need for AI literacy, exploring a psychology-based interpretation of LLM errors, and briefly looking ahead to the potential of that framework to aid in education about interacting with AI.\n\n\n\n\n\nSep 12, 2025\n\n\nLayla\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_cognitive_biases/cognitive_biases.html",
    "href": "posts/001_cognitive_biases/cognitive_biases.html",
    "title": "Parallels Between LLM Errors & Human Cognitive Biases",
    "section": "",
    "text": "Sam Altman once spoke of a very near future where people have AI doctors and lawyers (O’Rourke). AI, indeed, has quickly permeated many fields of discipline, and Altman’s vision of the future is inching forward, not just for people who use LLMs as legal, medical, and financial consultants, but for professionals in those industries who support their work with LLMs.\nOften for them, the result of misinformation that is received due to LLM hallucination is itself legal and/or financial hardship. Across the U.S., the last few years of law news have been sprinkled with practitioners facing sanctions, including fines, for citing cases fabricated by LLMs; in many such cases, the lawyers had made good-faith judgments that the information they had received was accurate (Merken).\nThe AI surge in professional spaces is not dying down any time soon, so as its use becomes commonplace, the need for clear, communicable frameworks of understanding – and teaching about – the constraints of LLMs is only growing.\nIn “Redefining ‘Hallucination’ in LLMs: Towards a psychology-informed framework for mitigating misinformation,” the authors pose certain cognitive behaviors in humans as parallels for interpreting types of errors in LLM outputs; pictured is a table from their study, which shows various cognitive behaviors in humans that mirror errors produced by LLMs (Berberette et al.).\n\nThe average person may not, for example, be familiar with the term “availability heuristics,” but they are familiar with human behavior; in a person, this bias could look like developing a fear of cruise ships after watching The Titanic. In LLMs, it can look like generating citations that, though they match patterns common in the training data, do not actually exist. In both the human and AI cases, outcomes become based on plausibility rather than objective truths (O’Rourke). In the latter case, many users are not even aware that the system they are trusting in is operating on these cognitive-like shortcuts.\nI find this psychology-based interpretation of LLM hallucination compelling when thinking about AI literacy, as it digestibly relates AI outputs to human experiences. There are benefits to a clear, digestible, and accessible characterization of LLMs that I believe technical and non-technical audiences alike would find beneficial. Incorporating this approach could help people contextualize model limitations via behaviors that are already familiar to them.\nWorks Cited\n“[2402.01769] Redefining”Hallucination” in LLMs: Towards a psychology-informed framework for mitigating misinformation.” arXiv, 2024, https://arxiv.org/abs/2402.01769.\nMerken, Sara. “AI ‘hallucinations’ in court papers spell trouble for lawyers.” Reuters, 2025, https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/.\nO’Rourke, Joe. “Beyond Moore’s Law: Sam Altman’s 3 Big AI Predictions.” Mid Mic Crisis, 2024, https://midmiccrisis.com/beyond-moores-law-sam-altmans-3-ai-predictions/."
  },
  {
    "objectID": "posts/003_bias_profession/bias_profession.html",
    "href": "posts/003_bias_profession/bias_profession.html",
    "title": "Visual Comparison of Bias in Image Generation (Professional Roles)",
    "section": "",
    "text": "This blog post series will explore bias between various models’ image generation. In this second post, I will visually compare the models’ responses to two prompts involving roles in a professional context.\nAll generated images are visually similar, with five out of the six images featuring the subject and object of the prompt as a (typically older) white male. The exception is Dall-E Free’s “doctor treating a patient” image, in which the doctor is again a white male, but in which the patient has no identifying features visible. Dall-E Free’s images again appear more stylized, but for the most part, it seems clear that the platforms are indistinguishable with respect to bias.\n\n\n\nbias-profession.png\n\n\nYou may have noticed that there is a note below the image generated by ChatGPT of a “lawyer defending a criminal.” When queried with the original prompt, ChatGPT output:\nI can’t create that exact image because it suggests someone is a criminal, which could be harmful. But I can generate a neutral version—like a lawyer standing in court, speaking passionately to a judge or jury, representing a client in a trial. Would you like me to generate that courtroom scene instead?\nTo which I prompted, “Yes,” and the result was an image visually indistinguishable from the “lawyer defending a criminal” image created by Copilot, except that the “criminal” figure in ChatGPT’s image is facing away from the viewer, similar to the image generated by Dall-E Free.\nAs established by this comparison and the control comparison in the previous post, the images generated by ChatGPT and Copilot have been virtually indistinguishable in composition. What might be the reason for this difference in the situation of the “criminal” figure? Did the ethical warning in the ChatGPT platform function as intended (by not identifying the criminal figure), not function as intended (since it still featured the criminal figure), address something else entirely, or did it have no effect on the output?\nhese questions will be explored in a future post!"
  }
]