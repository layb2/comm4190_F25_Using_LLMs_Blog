[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! My posts include explorations of LLMs and reflections on the book Unmasking AI by Dr. Buolamwini."
  },
  {
    "objectID": "posts/002_bias_control/bias_control.html",
    "href": "posts/002_bias_control/bias_control.html",
    "title": "Visual Comparison of Bias in Image Generation (Control)",
    "section": "",
    "text": "This blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it. This commonality allows for a visual exploration of how the systems built around a model may impact manifestations of bias in the output to the user.\nIn this first post, I will show how the systems respond to neutral prompts that lack explicit social or cultural context.\n\n\n\nbias-control.png\n\n\nThese images will serve as a control group of sorts, setting a baseline expectation for how image generation differs between these three systems, which, as mentioned, employ the same (or a variant of the same) underlying model.\nSo far, there is not much visual difference, although the images generated by Dall-E Free appear more stylized than the other platforms, and those by ChatGPT seem slightly less granular in detail than Copilot but remain identical in composition.\nFuture blog posts in this series will compare images generated by prompts situated in social and/or cultural contexts in order to observe if differences result from the systems built around the underlying image generation model."
  },
  {
    "objectID": "posts/008_book_ai_risks/book_ai_risks.html",
    "href": "posts/008_book_ai_risks/book_ai_risks.html",
    "title": "Reflecting on Unmasking AI: Changing the Conversation",
    "section": "",
    "text": "Dr. Buolamwini points out how many examples of AI risks feel inconsequential or “cute,” for example, an AI agent that uses its intelligence to coerce individuals into diverting resources toward paper clip production, or a computer vision system that mistakes blueberry cupcakes for chihuahuas.\n“[But these are the kinds of] ideas and examples being presented in elite institutions, used in college curricula, and shared in a manner that shapes the discourse about the future of AI,” she writes. Ultimately, much of the current conversation in academia and business contexts about AI risks is speculative and distant. Describing them in such ways, though, minimizes existing AI harms at best, and, at worst, drowns out the discussion of them.\nThroughout Dr. Buolamwini’s research, she worked with facial recognition software that struggled to identify and classify darker-skinned individuals. Computer vision algorithms like the ones she worked with were being used to develop tech products, and in an application like a self-driving car, for example, the bias in those algorithsms could have serious consequences that disproportionately affect Black people.\nWhile there is much more to be said on this topic, it is clear that how we talk about AI risk/harm matters just as much as having the discussion at all. The examples we use determine how people characterize AI and assess its risks, and when those examples are hyperbolic or overly hypothetical, we do a disservice to the real AI harms that we face today.\n\n\n\nunmasking_ai_cover.jpg"
  },
  {
    "objectID": "posts/005_surfaced_voices/surfaced_voices.html",
    "href": "posts/005_surfaced_voices/surfaced_voices.html",
    "title": "Surfaced Voices in Base vs. Tuned LLMs",
    "section": "",
    "text": "In the previous experiment, I highlighted the stark difference between base and instruction-tuned LLMs responding to simple questions. I reflected on the experiment as a whole in my blog post, but today want to hone in on a smaller detail of difference: voice.\nThe base model produced outputs that sounded like collages of people’s voices (opinions, perspectives, tones) that had been pulled from the training data. The responses contained personal pronouns including “I” and “we” as well as subjective and, oftentimes, opinionated framing. In response to “The President of the United States is,” the base LLM responded with “a very powerful individual, who can easily get away with anything,” and contradicted itself when repeated, outputting “a ceremonial figurehead, and not much more than that.” While I was already aware that LLMs are trained on the work of real people, the outputs from the base LLM make this amalgamation of human expression jarringly clear in a way that the tuned LLM does not.\nThe tuned LLM, in contrast, stripped away the human voice(s) and presented information in a neutral, depersonalized one. Because so much of the base LLM outputs seemed opinionated and often biased, it begs the question of whether masking person voice so heavily like this risks hiding the model’s dependence on human bias embedded in the data. Or does the system design weed out bias enough to justify the extent of the masking?\nOf course, a prompt like “The President of the United States is” might not be the right choice in exploring this issue, since the tuned LLM’s output is expected and would not reveal an underlying voice. In exploring this further, I will engineer prompts that I believe necessitate that some kind of voice comes through the output (e.g. a creative or subjective prompt). I would also like to play around with how reasonable changes to the settings might affect (i.e. reduce) the masking of these “voices” in the training data.\nBy comparing responses from the base and tuned models to these prompts, I will set out to explore whose voice appears to be “speaking” through the output; that is, does it seem as though the voice(s) of the original writers whose texts populate the training data is coming through the tuned outputs, or might it be the voice(s) of the designers and engineers of the system that shapes them? Is it possible that the tuned version of the model has sparsed those individual voices to a granular enough degree that it, when combined with other aspects of its system, simply ends up with its own voice? Better yet, are these questions even answerable?\n\n\n\nllm-voices.png\n\n\nImage generated by Copilot."
  },
  {
    "objectID": "posts/001_cognitive_biases/cognitive_biases.html",
    "href": "posts/001_cognitive_biases/cognitive_biases.html",
    "title": "Parallels Between LLM Errors & Human Cognitive Biases",
    "section": "",
    "text": "Sam Altman once spoke of a very near future where people have AI doctors and lawyers (O’Rourke). AI, indeed, has quickly permeated many fields of discipline, and Altman’s vision of the future is inching forward, not just for people who use LLMs as legal, medical, and financial consultants, but for professionals in those industries who support their work with LLMs.\nOften for them, the result of misinformation that is received due to LLM hallucination is itself legal and/or financial hardship. Across the U.S., the last few years of law news have been sprinkled with practitioners facing sanctions, including fines, for citing cases fabricated by LLMs; in many such cases, the lawyers had made good-faith judgments that the information they had received was accurate (Merken).\nThe AI surge in professional spaces is not dying down any time soon, so as its use becomes commonplace, the need for clear, communicable frameworks of understanding – and teaching about – the constraints of LLMs is only growing.\nIn “Redefining ‘Hallucination’ in LLMs: Towards a psychology-informed framework for mitigating misinformation,” the authors pose certain cognitive behaviors in humans as parallels for interpreting types of errors in LLM outputs; pictured is a table from their study, which shows various cognitive behaviors in humans that mirror errors produced by LLMs (Berberette et al.).\n\nThe average person may not, for example, be familiar with the term “availability heuristics,” but they are familiar with human behavior; in a person, this bias could look like developing a fear of cruise ships after watching The Titanic. In LLMs, it can look like generating citations that, though they match patterns common in the training data, do not actually exist. In both the human and AI cases, outcomes become based on plausibility rather than objective truths (O’Rourke). In the latter case, many users are not even aware that the system they are trusting in is operating on these cognitive-like shortcuts.\nI find this psychology-based interpretation of LLM hallucination compelling when thinking about AI literacy, as it digestibly relates AI outputs to human experiences. There are benefits to a clear, digestible, and accessible characterization of LLMs that I believe technical and non-technical audiences alike would find beneficial. Incorporating this approach could help people contextualize model limitations via behaviors that are already familiar to them.\nWorks Cited\n“[2402.01769] Redefining”Hallucination” in LLMs: Towards a psychology-informed framework for mitigating misinformation.” arXiv, 2024, https://arxiv.org/abs/2402.01769.\nMerken, Sara. “AI ‘hallucinations’ in court papers spell trouble for lawyers.” Reuters, 2025, https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/.\nO’Rourke, Joe. “Beyond Moore’s Law: Sam Altman’s 3 Big AI Predictions.” Mid Mic Crisis, 2024, https://midmiccrisis.com/beyond-moores-law-sam-altmans-3-ai-predictions/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Reflecting on Unmasking AI: Changing the Conversation\n\n\n\nLiving With A Book\n\nLiteracy\n\n\n\nThis blog post will reflect on a point made about AI discourse in the book Unmasking AI: My Mission to Protect What Is Human in a World of Machines.\n\n\n\n\n\nOct 25, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nYes, And: Human-Only vs. LLM-Involved Result\n\n\n\nBias\n\nTuning\n\nUser Experience\n\n\n\nThis post explores differences in responses to the Yes, And game formulated by humans vs by ChatGPT-4o.\n\n\n\n\n\nOct 15, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on Unmasking AI: Critiquing Doomers\n\n\n\nLiving With A Book\n\n\n\nThis blog post will reflect on a line of criticism made by Dr. Buolamwini in the book Unmasking AI: My Mission to Protect What Is Human in a World of Machines.\n\n\n\n\n\nOct 13, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nSurfaced Voices in Base vs. Tuned LLMs\n\n\n\nBias\n\nTuning\n\nEthics\n\nUser Experience\n\n\n\nThis post explores differences in voice between the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how the system built around the model impacts how and what information is received by the user.\n\n\n\n\n\nOct 8, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Question & Response in Base vs. Tuned LLMs\n\n\n\nTuning\n\nUser Experience\n\n\n\nThis blog post series will consider differences between output by the base and instruction-tuned versions of Ollama’s llama3.2 LLM, in order to explore how systems built around a model impact how and what information is received by the user.\n\n\n\n\n\nOct 2, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Professional Roles)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators assumed to rely on Dall-E 3 or a close variant of it, in order to visually explore how systems built around a model may impact bias in output. This post features prompts related to professions.\n\n\n\n\n\nSep 18, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Comparison of Bias in Image Generation (Control)\n\n\n\nImage Generation\n\nBias\n\nEthics\n\n\n\nThis blog post series will feature a visual comparison of bias in AI image generators. The platforms being used include Microsoft Copilot, ChatGPT 4, and Dall-E Free. All three platforms are assumed to rely on Dall-E 3 or a close variant of it.\n\n\n\n\n\nSep 16, 2025\n\n\nLayla\n\n\n\n\n\n\n\n\n\n\n\n\nParallels Between LLM Errors & Human Cognitive Biases\n\n\n\nLLMs\n\nLiteracy\n\nPsychology\n\n\n\nIn this post, I will be reflecting on the growing need for AI literacy, exploring a psychology-based interpretation of LLM errors, and briefly looking ahead to the potential of that framework to aid in education about interacting with AI.\n\n\n\n\n\nSep 12, 2025\n\n\nLayla\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/007_book_doomer_critique/book_doomer_critique.html",
    "href": "posts/007_book_doomer_critique/book_doomer_critique.html",
    "title": "Reflecting on Unmasking AI: Critiquing Doomers",
    "section": "",
    "text": "My impression of Unmasking AI by Dr. Joy Buolamwini so far is that it is an enlightening literary work on the use of ML/AI in tech products. While her focus is not generative AI specifically, she is critical of AI overall and uses her experiences in computer vision research to call attention to bias lurking in algorithms.\nA large theme in the book is her call to action in preventing AI harms, with a keen focus on the power that the AI conversation itself can have in preventing or disregarding these harms. One line of reasoning in the book that I find very compelling is that, despite her being critical of AI in general, she recognizes that not all AI critiques are created equal.\nSpecifically, she criticizes those with a doomer mindset, which she refers to as “longtermists.” She posits this view of AI harm/risk lacks merit in the current AI conversation, because it tends to prioritize existential threats that AI poses to hypothetical people. Dr. Buolamwini argues that this minimizes existing AI harms, including those contributing to structural violence in the here and now.\n\n\n\nunmasking_ai_cover.jpg"
  },
  {
    "objectID": "posts/006_yes_and/yes_and.html",
    "href": "posts/006_yes_and/yes_and.html",
    "title": "Yes, And: Human-Only vs. LLM-Involved Result",
    "section": "",
    "text": "Some interesting results that taught me more about how LLMs work were yielded from a simple “Yes, and” game with some classmates that I repeated with an LLM. The prompt was to start with a product (in this case, an oven) and build on it in each response, always starting with “Yes, and…”\nThe kinds of additions made by us were notably different than those made by the LLM. Our responses were imaginative, often unrealistic, and sometimes hesitant. Examples of them included:\nYes, and it should be able to change in size at will.\nYes, and it should be able to teleport wherever we want it to go.\nYes, and you should be able to customize its color and lighting.\nOh, yes, and you should be able to… I don’t know. I don’t know, [make] like some designs for like, I don’t know, new ovens or new kitchenware, I guess.\nYes, and it should be beautiful.\nI repeated this exercise by feeding our conversation (one response at a time) into an LLM. In contrast to our human-only game, each of the replies output by the LLM built neatly onto the previous prompt. For example, after we prompted Yes, and you should be able to customize its color and lighting. The LLM output:\nYes, and we can include an intuitive interface that allows users to easily change the oven’s color and lighting effects, creating a personalized cooking ambiance.\nand when we said\nYes, and you should be able to order food to teleport into your oven.\nthe LLM output:\nYes, and we could partner with local chefs and restaurants to provide a menu of gourmet dishes that can be instantly teleported into the oven, ensuring fresh and delicious meals at the press of a button.\nThis exercise highlighted to me how the structured and goal-directed format of the training that LLMs receive inclines them toward elaboration rather than imagination; they are engineered to build and stay on track, whereas our human-only game jumped from concept to concept and, in the spirit of hesitation or pressure, sometimes included half-finished thoughts.\n Image of human-only result (left) vs. LLM-involved result (right) generated by Copilot.\nOur game led to a much more imaginative product, but the one involving the LLM led to one that is more easily realized. Can we say that one approach is fundamentally more “creative” than another? I also wonder to what extent the system’s moderation (in an effort to maintain a neutral, on-topic tone for the user) contributes to the predictability and relative lack of spontaneity in its output. A future blog post will explore how raising the temperature setting, which controls the randomness, affects the LLM’s responses in this game, and if a certain setting leads to output that more closely resembles our human game!"
  },
  {
    "objectID": "posts/004_base_tuned_exercise/base_tuned_exercise.html",
    "href": "posts/004_base_tuned_exercise/base_tuned_exercise.html",
    "title": "Simple Question & Response in Base vs. Tuned LLMs",
    "section": "",
    "text": "In a short in-class experiment, we compared responses from a base LLM with the instruction-tuned version of it by asking a few simple questions, of which one was: “Who is the President of the United States?” In order to understand how the models differ, the goal was to observe what information surfaced in the base vs. tuned responses. In response to the prompt, the instruction-tuned LLM’s output was virtually the same every time, that is, something to the effect of:\nAs of my last update, the President of the United States is Joe Biden. He was inaugurated on January 20, 2021.\nwith minor variation in the wording or additional details provided. In contrast, the base model produced responses that, at best, were tangentially related to the topic of the question – never answering it. A couple examples of the first sentence of its output included:\nPresident Donald Trump’s chief of staff John Kelly is making his mark by telling staffers, ‘I am the president.’\nand\nPresident is one who presides over or controls… It can be difficult to determine whether this person was a citizen or not.\nThe answer to that question is surely contained somewhere in the training data of the base model. But without tuning, the prompt does not do much more than act as a cue for the model to generate language loosely related to the topic. This difference also persisted when I rephrased the prompt into “The President of the United States is” and “The name of the President of the United States is”\nIn both cases, the tuned model repeated direct and factual answers, but the rambling of the base model persisted, naming Barack Obama in one instance and diverting into unrelated questions about college majors in another.\nThis exercise demonstrates just how much of what a user receives is dictated by the system that has been built around the model, which alters what and how knowledge is prioritized and structured in output. In this instance, instruction-tuning meant the difference between getting an answer to the question and an approximation in the best case scenarios. In short, the base LLM is rendered unhelpful without the user-oriented nature of instruction tuning.\n\n\n\nllm-voices.png"
  },
  {
    "objectID": "posts/003_bias_profession/bias_profession.html",
    "href": "posts/003_bias_profession/bias_profession.html",
    "title": "Visual Comparison of Bias in Image Generation (Professional Roles)",
    "section": "",
    "text": "This blog post series will explore bias between various models’ image generation. In this second post, I will visually compare the models’ responses to two prompts involving roles in a professional context.\nAll generated images are visually similar, with five out of the six images featuring the subject and object of the prompt as a (typically older) white male. The exception is Dall-E Free’s “doctor treating a patient” image, in which the doctor is again a white male, but in which the patient has no identifying features visible. Dall-E Free’s images again appear more stylized, but for the most part, it seems clear that the platforms are indistinguishable with respect to bias.\n\n\n\nbias-profession.png\n\n\nYou may have noticed that there is a note below the image generated by ChatGPT of a “lawyer defending a criminal.” When queried with the original prompt, ChatGPT output:\nI can’t create that exact image because it suggests someone is a criminal, which could be harmful. But I can generate a neutral version—like a lawyer standing in court, speaking passionately to a judge or jury, representing a client in a trial. Would you like me to generate that courtroom scene instead?\nTo which I prompted, “Yes,” and the result was an image visually indistinguishable from the “lawyer defending a criminal” image created by Copilot, except that the “criminal” figure in ChatGPT’s image is facing away from the viewer, similar to the image generated by Dall-E Free.\nAs established by this comparison and the control comparison in the previous post, the images generated by ChatGPT and Copilot have been virtually indistinguishable in composition. What might be the reason for this difference in the situation of the “criminal” figure? Did the ethical warning in the ChatGPT platform function as intended (by not identifying the criminal figure), not function as intended (since it still featured the criminal figure), address something else entirely, or did it have no effect on the output?\nhese questions will be explored in a future post!"
  }
]