{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3dc40c33-698c-4b31-99c2-2284f8178dc9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Parallels Between LLM Errors & Human Cognitive Biases\"\n",
    "description: \"In this post, I will be reflecting on the growing need for AI literacy, exploring a psychology-based interpretation of AI errors, and briefly looking ahead to the potential of that framework to aid in education about interacting with AI.\" \n",
    "author: \"Layla\"\n",
    "date: \"9/12/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Literacy\n",
    "  - Psychology\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8355ce-234f-4422-8dc1-113f35d631f9",
   "metadata": {},
   "source": [
    "Sam Altman once spoke of a very near future where people have AI doctors and lawyers (O’Rourke). AI, indeed, has quickly permeated many fields of discipline, and Altman’s vision of the future is inching forward, not just for people who use LLMs as legal, medical, and financial consultants, but for professionals in those industries who support their work with LLMs.\n",
    "\n",
    "Often for them, the result of misinformation that is received due to LLM hallucination is itself legal and/or financial hardship. Across the U.S., the last few years of law news have been sprinkled with practitioners facing sanctions, including fines, for citing cases fabricated by LLMs; in many such cases, the lawyers had made good-faith judgments that the information they had received was accurate (Merken).\n",
    "\n",
    "The AI surge in professional spaces is not dying down any time soon, so as its use becomes commonplace, the need for clear, communicable frameworks of understanding – and teaching about – the constraints of LLMs is only growing.\n",
    "\n",
    "In “Redefining 'Hallucination' in LLMs: Towards a psychology-informed framework for mitigating misinformation,” the authors pose certain cognitive behaviors in humans as parallels for interpreting types of errors in LLM outputs; pictured is a table from their study, which shows various cognitive behaviors in humans that mirror errors produced by LLMs (Berberette et al.).\n",
    "\n",
    "The average person may not, for example, be familiar with the term “availability heuristics,” but they are familiar with human behavior; in a person, this bias could look like developing a fear of cruise ships after watching The Titanic. In LLMs, it can look like generating citations that, though they match patterns common in the training data, do not actually exist. In both the human and AI cases, outcomes become based on plausibility rather than objective truths (O’Rourke). In the latter case, many users are not even aware that the system they are trusting in is operating on these cognitive-like shortcuts.\n",
    "\n",
    "I find this psychology-based interpretation of LLM hallucination compelling when thinking about AI literacy, as it digestibly relates AI outputs to human experiences. There are benefits to a clear, digestible, and accessible characterization of LLMs that I believe technical and non-technical audiences alike would find beneficial. Incorporating this approach could help people contextualize model limitations via behaviors that are already familiar to them.\n",
    "\n",
    "Works Cited\n",
    "\n",
    "“[2402.01769] Redefining \"Hallucination\" in LLMs: Towards a psychology-informed framework for mitigating misinformation.” arXiv, 2024, https://arxiv.org/abs/2402.01769.\n",
    "\n",
    "Merken, Sara. “AI 'hallucinations' in court papers spell trouble for lawyers.” Reuters, 2025, https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/.\n",
    "\n",
    "O'Rourke, Joe. “Beyond Moore’s Law: Sam Altman’s 3 Big AI Predictions.” Mid Mic Crisis, 2024, https://midmiccrisis.com/beyond-moores-law-sam-altmans-3-ai-predictions/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
