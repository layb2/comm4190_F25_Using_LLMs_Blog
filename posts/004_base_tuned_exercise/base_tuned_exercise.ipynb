{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9a552e44-3bb9-442f-af57-f2f38edcc15e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Simple Question & Response in Base vs. Tuned LLMs\"\n",
    "description: \"This blog post series will consider differences between output by the base and instruction-tuned versions of Ollama's llama3.2 LLM, in order to explore how systems built around a model impact how and what information is received by the user.\" \n",
    "author: \"Layla\"\n",
    "date: \"10/2/2025\"\n",
    "categories:\n",
    "  - Bias\n",
    "  - Tuning\n",
    "  - Ethics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066ec9e-13be-4122-8622-4004ebcdcb9a",
   "metadata": {},
   "source": [
    "In a short in-class experiment, we compared responses from a base LLM with the instruction-tuned version of it by asking a few simple questions, of which one was: \"Who is the President of the United States?\"\n",
    "In order to understand how the models differ, the goal was to observe what information surfaced in the base vs. tuned responses. In response to the prompt, the instruction-tuned LLM’s output was virtually the same every time, that is, something to the effect of:\n",
    "\n",
    "**As of my last update, the President of the United States is Joe Biden. He was inaugurated on January 20, 2021.**\n",
    "\n",
    "with minor variation in the wording or additional details provided. In contrast, the base model produced responses that, at best, were tangentially related to the topic of the question – never answering it. A couple examples of the first sentence of its output included:\n",
    "\n",
    "**President Donald Trump’s chief of staff John Kelly is making his mark by telling staffers, ‘I am the president.’**\n",
    "\n",
    "and\n",
    "\n",
    "**President is one who presides over or controls... It can be difficult to determine whether this person was a citizen or not.**\n",
    "\n",
    "The answer to that question is surely contained somewhere in the training data of the base model. But without tuning, the prompt does not do much more than act as a cue for the model to generate language loosely related to the topic.\n",
    "This difference also persisted when I rephrased the prompt into \"The President of the United States is\" and \"The name of the President of the United States is\"\n",
    "\n",
    "In both cases, the tuned model repeated direct and factual answers, but the rambling of the base model persisted, naming Barack Obama in one instance and diverting into unrelated questions about college majors in another.\n",
    "\n",
    "This exercise demonstrates just how much of what a user receives is dictated by the system that has been built around the model, which alters what and how knowledge is prioritized and structured in output. In this instance, instruction-tuning meant the difference between getting an answer to the question and an approximation in the best case scenarios. In short, the base LLM is rendered unhelpful without the user-oriented nature of instruction tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
